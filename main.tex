% ============================================================================
% FL-EHDS Paper for FLICS 2026
% IEEE Conference Format - IMPROVED VERSION
% Version: 2.0 - February 2026
% ============================================================================

\documentclass[conference]{IEEEtran}

% ===== PACKAGES =====
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{url}

% TikZ for vector figures
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, fit, backgrounds, calc}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% ===== DOCUMENT =====
\begin{document}

\title{FL-EHDS: A Privacy-Preserving Federated Learning Framework for the European Health Data Space}

\author{
    \IEEEauthorblockN{Fabio Liberti}
    \IEEEauthorblockA{
        Department of Computer Science\\
        Universitas Mercatorum, Rome, Italy\\
        fabio.liberti@unimercatorum.it\\
        ORCID: 0000-0003-3019-5411
    }
}

\maketitle

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
The European Health Data Space (EHDS), established by Regulation (EU) 2025/327 and effective March 2025, mandates cross-border health data analytics while preserving citizen privacy. Federated Learning (FL) emerges as the key enabling technology for secondary use, yet systematic evidence synthesis reveals critical implementation gaps: only 23\% of FL implementations achieve sustained production deployment in healthcare settings, with hardware heterogeneity (78\%) and non-IID data distributions (67\%) as dominant technical barriers. Legal uncertainties regarding gradient data status under GDPR and controller/processor responsibilities remain unresolved. We present FL-EHDS, a three-layer compliance framework integrating governance mechanisms (Health Data Access Bodies, data permits, opt-out registries), FL orchestration (aggregation within Secure Processing Environments, differential privacy), and data holder components (adaptive training, FHIR preprocessing). The framework maps evidence-based barriers to specific mitigation strategies and provides compliance checkpoints aligned with EHDS requirements. This paper contributes: (1) the first systematic barrier taxonomy for FL in EHDS contexts based on 47 documents following PRISMA methodology; (2) a reference architecture addressing identified technical, legal, and organizational gaps; (3) an open-source reference implementation providing modular components for practical deployment; (4) an implementation roadmap for the critical 2025-2031 transition period with prioritized actions for policymakers, national authorities, and healthcare organizations.
\end{abstract}

\begin{IEEEkeywords}
Federated Learning, European Health Data Space, Privacy-Preserving Technologies, GDPR, Health Data Governance, Cross-Border Analytics, Differential Privacy
\end{IEEEkeywords}

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}
\label{sec:introduction}

The European Health Data Space (EHDS), established by Regulation (EU) 2025/327, represents the European Union's most ambitious initiative for cross-border health data governance~\cite{eu2025ehds}. Entering into force on 26 March 2025, the regulation creates a dual framework: primary use through MyHealth@EU infrastructure for direct patient care, and secondary use through HealthData@EU for research, innovation, and evidence-based policy-making~\cite{hussein2025interop}.

The EHDS introduces novel governance mechanisms of unprecedented complexity. Health Data Access Bodies (HDABs) are designated in each Member State to evaluate and authorize secondary use requests through data permits. Article 53 enumerates permitted purposes including scientific research, public health surveillance, and AI training; Article 71 introduces opt-out mechanisms allowing citizens to object to secondary use of their electronic health data~\cite{staunton2024ethical}. The implementation timeline extends to 2031, with delegated acts expected by March 2027 and secondary use provisions applicable from March 2029.

\subsection{The Technology-Governance Divide}

Federated Learning (FL) emerges as the theoretically ideal technical solution for EHDS secondary use---the model travels to distributed data sources rather than centralizing sensitive health records~\cite{rieke2020future}. The COVID-19 pandemic demonstrated FL's potential at scale: Dayan et al.~\cite{dayan2021federated} trained a global model across 20 institutions in 5 countries, achieving robust predictions without data centralization. This ``data stays home'' principle aligns with GDPR data minimization requirements and addresses legitimate concerns about health data sovereignty across 27 Member States~\cite{mcmahan2017communication}.

However, recent evidence reveals a sobering gap between FL's theoretical promise and operational reality. Fr\"ohlich et al.~\cite{frohlich2025reality} report that only 23\% of reviewed FL implementations achieve sustained production deployment in healthcare settings. Technical barriers persist: hardware heterogeneity affects 78\% of pilot participants; non-IID data challenges impact 67\% of tested models. Beyond technical constraints, legal uncertainties regarding gradient data status under GDPR and controller/processor responsibilities in FL architectures remain unresolved~\cite{quinn2024gdpr}, creating compliance risks that discourage organizational adoption.

Van Drumpt et al.~\cite{vandrumpt2025pets} demonstrate through expert interviews that privacy-enhancing technologies cannot substitute for robust governance frameworks---public trust depends primarily on institutional transparency and accountability rather than technical privacy guarantees alone.

\subsection{Contributions}

This paper bridges the technology-governance divide by making four contributions:

\begin{enumerate}
    \item \textbf{Barrier Taxonomy}: Systematic evidence synthesis of FL implementation barriers specific to EHDS contexts (47 documents, PRISMA methodology, GRADE-CERQual confidence assessment).
    \item \textbf{FL-EHDS Framework}: A three-layer reference architecture with compliance checkpoints mapping barriers to mitigation strategies.
    \item \textbf{Reference Implementation}: Open-source modular Python codebase implementing the framework components for practical deployment.
    \item \textbf{Implementation Roadmap}: Prioritized actions for the 2025-2031 transition period addressing policymakers, national authorities, and healthcare organizations.
\end{enumerate}

% ============================================================================
% 2. BACKGROUND AND RELATED WORK
% ============================================================================
\section{Background and Related Work}
\label{sec:background}

\subsection{European Health Data Space}

The EHDS establishes HDABs in each Member State to authorize secondary use through standardized data permits. Secure Processing Environments (SPEs) provide controlled settings for analytics without data leaving institutional boundaries~\cite{svingel2025hdab}. Table~\ref{tab:timeline} presents the implementation timeline with FL-specific relevance.

\begin{table}[htbp]
\caption{EHDS Implementation Timeline}
\label{tab:timeline}
\centering
\small
\begin{tabular}{lll}
\toprule
\textbf{Date} & \textbf{Milestone} & \textbf{FL Relevance} \\
\midrule
Mar 2025 & Entry into force & Legal framework active \\
Mar 2027 & Delegated acts & Gradient status clarification \\
Mar 2029 & Secondary use application & FL must be operational \\
Mar 2031 & Genetic, imaging data & Extended FL requirements \\
\bottomrule
\end{tabular}
\end{table}

Forster et al.~\cite{forster2025journeys} document significant variability in current data access experiences across Member States, with timelines ranging from 3 weeks (Finland) to over 12 months (France). Critically, barriers are primarily organizational and procedural rather than technical, suggesting that infrastructure investments alone will not resolve access inequities.

\subsection{Federated Learning Fundamentals}

FL inverts the traditional machine learning paradigm: rather than centralizing data, the model travels to distributed sources~\cite{mcmahan2017communication}. Local training produces gradients; these are aggregated centrally (typically via FedAvg or FedProx algorithms) and redistributed for iterative refinement~\cite{li2020federated, kairouz2021advances}. Known challenges include: non-IID data distributions causing convergence difficulties~\cite{li2020federated}; communication costs for gradient exchange~\cite{bonawitz2019scale}; and privacy attacks including gradient inversion~\cite{zhu2019deep} and membership inference~\cite{shokri2017membership}.

Teo et al.~\cite{teo2024systematic} conducted a comprehensive systematic review of FL in healthcare (612 articles), finding that the majority remain proof-of-concept studies with only 5.2\% achieving real-life application. This maturity gap has direct implications for EHDS timelines.

\subsection{Related Work}

Prior FL frameworks for healthcare~\cite{rieke2020future, peng2024systematic} focus on technical architectures without addressing regulatory compliance in specific jurisdictions. Legal analyses~\cite{quinn2024gdpr, staunton2024ethical} examine GDPR constraints but abstract from implementation feasibility. Policy documents from TEHDAS~\cite{tehdas2024ready} assess Member State readiness but do not integrate technical FL considerations.

FL-EHDS uniquely bridges these dimensions by: (1) grounding the framework in systematic evidence synthesis; (2) explicitly addressing EHDS regulatory requirements; and (3) mapping technical barriers to governance-aware mitigation strategies.

% ============================================================================
% 3. FL-EHDS FRAMEWORK
% ============================================================================
\section{FL-EHDS Framework}
\label{sec:framework}

We present FL-EHDS, a three-layer compliance framework designed for EHDS cross-border health analytics. The architecture addresses identified barriers while maintaining alignment with regulatory requirements.

\subsection{Architecture Overview}

Figure~\ref{fig:architecture} illustrates the FL-EHDS architecture comprising three integrated layers:

\begin{itemize}
    \item \textbf{Layer 1 (Governance)}: HDAB integration, data permit verification, opt-out registry synchronization, compliance audit logging.
    \item \textbf{Layer 2 (FL Orchestration)}: Aggregation within SPE boundaries, privacy protection modules (differential privacy, gradient clipping), purpose limitation enforcement.
    \item \textbf{Layer 3 (Data Holders)}: Adaptive local training engines, FHIR preprocessing pipelines, secure gradient communication.
\end{itemize}

% ===== INCLUDE FIGURE FROM figures/ FOLDER =====
\input{figures/fig2-fl-ehds-architecture}

\subsection{Layer 1: Governance Layer}

\textbf{HDAB Integration}: Standardized APIs enable automated data permit verification before FL training initiation. Multi-HDAB synchronization protocols coordinate cross-border studies involving multiple Member States, addressing the coordination complexity identified by Christiansen et al.~\cite{christiansen2025pilot}.

\textbf{Opt-out Registry}: National opt-out registries are consulted before each training round, ensuring Article 71 compliance. The framework implements granular opt-out checking at the record level while maintaining performance through caching mechanisms.

\textbf{Compliance Logging}: Comprehensive audit trails satisfy GDPR Article 30 requirements, documenting data access, processing purposes, and model outputs for regulatory inspection.

\subsection{Layer 2: FL Orchestration Layer}

\textbf{Aggregation Module}: The framework implements multiple aggregation algorithms: FedAvg~\cite{mcmahan2017communication} as baseline, FedProx~\cite{li2020federated} with proximal regularization for non-IID robustness, SCAFFOLD~\cite{karimireddy2020scaffold} with control variates for variance reduction, and FedNova~\cite{wang2020tackling} for normalized averaging under heterogeneous local steps. Algorithm selection is configurable based on deployment requirements.

\textbf{Privacy Protection}: Differential privacy (DP) with configurable $\varepsilon$-budget provides formal privacy guarantees~\cite{dwork2014dp, abadi2016deep}. The implementation uses R\'enyi Differential Privacy (RDP)~\cite{mironov2017renyi} for tight composition accounting over multiple training rounds. For Gaussian mechanisms with noise scale $\sigma$, the RDP guarantee at order $\alpha$ is $\rho(\alpha) = \alpha/(2\sigma^2)$. Privacy amplification by subsampling is computed using exact RDP formulas, and conversion to $(\varepsilon,\delta)$-DP uses optimal order selection: $\varepsilon = \min_\alpha \{\rho(\alpha) + \log(1/\delta)/(\alpha-1)\}$.

Table~\ref{tab:rdp_comparison} validates the RDP advantage: for 100+ round training typical of EHDS cross-border studies, RDP provides 5--6$\times$ tighter privacy bounds than naive composition, enabling longer training with equivalent privacy guarantees.

\begin{table}[htbp]
\centering
\caption{RDP vs.\ Simple Composition ($\sigma$=1.0, $\delta$=10$^{-5}$)}
\label{tab:rdp_comparison}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Rounds} & \textbf{Simple} & \textbf{RDP} & \textbf{Improvement} \\
\midrule
30 & $\varepsilon$=145 & $\varepsilon$=42 & 3.5$\times$ \\
100 & $\varepsilon$=484 & $\varepsilon$=98 & 4.9$\times$ \\
200 & $\varepsilon$=969 & $\varepsilon$=173 & 5.6$\times$ \\
\bottomrule
\end{tabular}
\end{table}

Gradient clipping bounds individual contribution magnitude, mitigating gradient inversion attacks~\cite{zhu2019deep}. Membership inference defense mechanisms prevent determination of training set membership~\cite{shokri2017membership}.

\textbf{Purpose Limitation}: Technical enforcement of permitted purposes (Article 53) through model output filtering and use-case validation, preventing scope creep beyond authorized analytics.

\subsection{Layer 3: Data Holder Layer}

\textbf{Adaptive Training Engine}: Resource-aware model partitioning addresses hardware heterogeneity (78\% barrier prevalence). The engine dynamically adjusts batch sizes, model complexity, and synchronization frequency based on local computational capabilities.

\textbf{FHIR Preprocessing}: Data normalization pipelines ensure interoperability across heterogeneous EHR systems. Only 34\% of European healthcare providers achieve full FHIR compliance~\cite{hussein2025interop}; the preprocessing module bridges format gaps through automated transformation.

\textbf{Secure Communication}: End-to-end encrypted gradient transmission ensures no raw data leaves institutional boundaries. Certificate-based authentication validates participant identity within the FL consortium.

\subsection{Reference Implementation}

A modular Python implementation of the FL-EHDS framework is available as open-source software at:

\begin{center}
\url{https://github.com/FabioLiberti/FL-EHDS-FLICS2026}
\end{center}

\noindent The implementation provides: (1) orchestration modules implementing FedAvg, FedProx, SCAFFOLD, and FedNova aggregation with R\'enyi differential privacy accounting (validated experimentally in Section~\ref{sec:experiments}) and secure aggregation; (2) data holder utilities for adaptive training and FHIR R4 preprocessing; (3) reproducible benchmarks generating all experimental results.

\textbf{Note on governance components}: HDAB integration APIs, opt-out registry synchronization, and multi-HDAB coordination modules are implemented as interface stubs designed for future integration with national EHDS infrastructure as it becomes available (expected 2027--2029). Current stubs define the API contracts and data flows; production deployment will require binding to actual Member State services.

\subsection{Threat Model and Security Assumptions}

The FL-EHDS framework assumes an \textit{honest-but-curious} threat model for the aggregation server, which follows the protocol correctly but may attempt to infer information from observed gradients. Byzantine tolerance is provided for up to $f < n/3$ malicious clients through robust aggregation mechanisms (Krum, Trimmed Mean, Bulyan). Gradient inversion attacks~\cite{zhu2019deep, carlini2022privacy} are mitigated through differential privacy ($\varepsilon$-budget enforcement) and secure aggregation protocols. The framework does not defend against collusion between the aggregation server and a majority of clients, which would require additional cryptographic protections (e.g., homomorphic encryption or trusted execution environments) beyond current scope.

\subsection{EHDS Compliance Mapping}

Table~\ref{tab:compliance} maps FL-EHDS framework components to specific EHDS regulatory requirements, demonstrating how technical implementation addresses legal obligations.

\begin{table}[htbp]
\caption{EHDS Compliance Mapping}
\label{tab:compliance}
\centering
\small
\begin{tabular}{lp{2.3cm}p{2.8cm}}
\toprule
\textbf{Article} & \textbf{Requirement} & \textbf{FL-EHDS Component} \\
\midrule
Art. 33 & Secondary use authorization & HDAB API + Permit validation \\
Art. 46 & Cross-border processing & Multi-HDAB coordinator \\
Art. 50 & Secure Processing Environment & Aggregation within SPE \\
Art. 53 & Permitted purposes & Purpose limitation module \\
Art. 71 & Opt-out mechanism & Registry filtering \\
Art. 69 & Quality labels & \textit{Future: Quality scoring} \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
% 4. EXPERIMENTAL EVALUATION
% ============================================================================
\section{Experimental Evaluation}
\label{sec:experiments}

We evaluate the FL-EHDS framework through comprehensive experiments simulating cross-border healthcare analytics. All results are fully reproducible via the benchmark suite in the repository.

\subsection{Experimental Setup}

\textbf{Task}: Binary classification for cardiovascular event risk prediction using five clinical features: age, BMI, systolic blood pressure, glucose level, and cholesterol. Features are normalized and labels generated from a logistic risk model.

\textbf{Data Distribution}: We simulate 3--7 hospitals across EU Member States with configurable non-IID distributions. Each hospital has 300--500 patient records with hospital-specific demographic biases (e.g., varying age distributions by region). Non-IID degree ranges from 0 (IID) to 0.8 (highly heterogeneous).

\textbf{Model}: Logistic regression classifier trained via federated optimization. While simpler than deep learning models, this choice isolates FL algorithm behavior from model complexity confounds.

\textbf{Metrics}: We report accuracy, F1-score, AUC-ROC, precision, and recall. All experiments run 50 rounds with 3 local epochs per round.

\subsection{Comprehensive Results}

Table~\ref{tab:main_results} presents results across multiple configurations with five performance metrics.

\begin{table}[htbp]
\centering
\caption{Comprehensive Experimental Results}
\label{tab:main_results}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Configuration} & \textbf{Acc.} & \textbf{F1} & \textbf{AUC} & \textbf{Prec.} & \textbf{Rec.} \\
\midrule
FedAvg (IID) & 60.5\%$\pm$0.02 & 0.62$\pm$0.02 & 0.66$\pm$0.01 & 0.62 & 0.62 \\
FedAvg (Non-IID) & 60.9\%$\pm$0.02 & 0.61$\pm$0.01 & 0.66$\pm$0.01 & 0.62 & 0.61 \\
FedProx ($\mu$=0.1) & 60.9\%$\pm$0.02 & 0.62$\pm$0.01 & 0.66$\pm$0.01 & 0.62 & 0.61 \\
SCAFFOLD & 60.5\%$\pm$0.01 & 0.61$\pm$0.02 & 0.66$\pm$0.01 & 0.62 & 0.61 \\
FedNova & 60.7\%$\pm$0.02 & 0.62$\pm$0.01 & 0.66$\pm$0.01 & 0.62 & 0.61 \\
FedAvg + DP ($\varepsilon$=10) & 55.7\%$\pm$0.01 & 0.61$\pm$0.04 & 0.55$\pm$0.03 & 0.56 & 0.67 \\
FedAvg + DP ($\varepsilon$=1) & 55.1\%$\pm$0.01 & 0.59$\pm$0.04 & 0.55$\pm$0.01 & 0.56 & 0.65 \\
\bottomrule
\end{tabular}

\vspace{1mm}
\footnotesize{5 hospitals, 30 rounds, 3 local epochs, batch size 32. Gradient clipping $C$=1.0. Results are mean $\pm$ std over 3 runs.}
\end{table}

\subsection{Convergence Analysis}

Figure~\ref{fig:convergence} shows training convergence across IID and non-IID configurations.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=0.45\textwidth,
    height=4.5cm,
    xlabel={Round},
    ylabel={Test Accuracy},
    legend style={at={(0.02,0.98)}, anchor=north west, font=\scriptsize},
    grid=major,
    grid style={dashed, gray!30},
    xmin=0, xmax=30,
    ymin=0.55, ymax=0.70,
]
\addplot[blue, thick] coordinates {
    (1,0.614) (5,0.628) (10,0.626) (15,0.630) (20,0.626) (25,0.628) (30,0.628)
};
\addplot[red, thick, dashed] coordinates {
    (1,0.624) (5,0.630) (10,0.630) (15,0.636) (20,0.634) (25,0.636) (30,0.634)
};
\addplot[green!60!black, thick, dotted] coordinates {
    (1,0.624) (5,0.632) (10,0.630) (15,0.636) (20,0.634) (25,0.636) (30,0.636)
};
\legend{FedAvg (IID), FedAvg (Non-IID), FedProx}
\end{axis}
\end{tikzpicture}
\caption{Training convergence: Algorithm comparison under IID and non-IID data distributions.}
\label{fig:convergence}
\end{figure}

\subsection{Privacy-Utility Tradeoff}

Table~\ref{tab:privacy_utility} quantifies the privacy-utility tradeoff across differential privacy budgets.

\begin{table}[htbp]
\centering
\caption{Privacy-Utility Tradeoff Analysis}
\label{tab:privacy_utility}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{$\varepsilon$} & \textbf{Accuracy} & \textbf{F1} & \textbf{AUC} & \textbf{Acc.~Drop} \\
\midrule
$\infty$ (No DP) & 60.9\%$\pm$0.02 & 0.61$\pm$0.01 & 0.66$\pm$0.01 & --- \\
10 & 55.7\%$\pm$0.01 & 0.61$\pm$0.04 & 0.55$\pm$0.03 & 5.2pp \\
1 (Strong) & 55.1\%$\pm$0.01 & 0.59$\pm$0.04 & 0.55$\pm$0.01 & 5.8pp \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:privacy_tradeoff} visualizes the accuracy degradation with increasing privacy protection.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar,
    bar width=0.8cm,
    width=0.42\textwidth,
    height=4cm,
    ylabel={Accuracy (\%)},
    xlabel={Privacy Budget ($\varepsilon$)},
    symbolic x coords={$\infty$, 10, 1},
    xtick=data,
    ymin=50, ymax=65,
    nodes near coords,
    nodes near coords align={vertical},
    every node near coord/.append style={font=\tiny},
    grid=major,
    grid style={dashed, gray!30},
]
\addplot[fill=blue!70] coordinates {($\infty$,60.9) (10,55.7) (1,55.1)};
\end{axis}
\end{tikzpicture}
\caption{Privacy-utility tradeoff: accuracy vs.\ $\varepsilon$-budget (RDP accounting).}
\label{fig:privacy_tradeoff}
\end{figure}

\subsection{Federated vs.\ Centralized Baseline}

A critical question for FL adoption is: how much accuracy is sacrificed for privacy preservation? Table~\ref{tab:fl_vs_central} compares FL algorithms with a centralized baseline where all hospital data is pooled (upper bound, no privacy).

\begin{table}[htbp]
\centering
\caption{FL vs.\ Centralized Training Comparison}
\label{tab:fl_vs_central}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Approach} & \textbf{Accuracy} & \textbf{F1} & \textbf{AUC} & \textbf{Gap} \\
\midrule
Centralized (Upper Bound) & $0.731 \pm 0.009$ & $0.738 \pm 0.011$ & $0.806 \pm 0.002$ & --- \\
\midrule
FedAvg (IID) & $0.662 \pm 0.017$ & $0.729 \pm 0.010$ & $0.785 \pm 0.014$ & 9.4\% \\
FedAvg (Non-IID) & $0.596 \pm 0.065$ & $0.236 \pm 0.334$ & $0.777 \pm 0.033$ & 18.5\% \\
FL + DP ($\varepsilon$=10) & $0.543 \pm 0.054$ & --- & --- & 25.7\% \\
\bottomrule
\end{tabular}

\vspace{1mm}
\footnotesize{5 hospitals, 200 samples each, 30 FL rounds $\times$ 3 local epochs = 90 centralized epochs. Results: mean $\pm$ std over 3 seeds.}
\end{table}

\textbf{Key finding}: Under IID conditions, FL sacrifices only 9.4\% accuracy compared to centralized training---a modest cost for preserving data sovereignty across institutions. Non-IID distributions increase this gap to 18.5\%, and adding differential privacy ($\varepsilon$=10) brings it to 25.7\%. These results quantify the privacy-utility tradeoff inherent in federated approaches.

\subsection{FL Algorithm Comparison}

Table~\ref{tab:algorithms} compares state-of-the-art FL aggregation algorithms under non-IID conditions: FedAvg~\cite{mcmahan2017communication}, FedProx~\cite{li2020federated}, SCAFFOLD~\cite{karimireddy2020scaffold}, and FedNova~\cite{wang2020tackling}.

\begin{table}[htbp]
\centering
\caption{FL Algorithm Comparison on Non-IID Data}
\label{tab:algorithms}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Algorithm} & \textbf{Accuracy} & \textbf{F1} & \textbf{Client Std.} \\
\midrule
FedAvg & 60.9\%$\pm$0.02 & 0.61$\pm$0.01 & 0.018 \\
FedProx ($\mu$=0.1) & 60.9\%$\pm$0.02 & 0.62$\pm$0.01 & 0.017 \\
SCAFFOLD & 60.5\%$\pm$0.01 & 0.61$\pm$0.02 & 0.016 \\
FedNova & 60.7\%$\pm$0.02 & 0.62$\pm$0.01 & 0.017 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Scalability Analysis}

Table~\ref{tab:scalability} demonstrates framework scalability across varying numbers of participating hospitals.

\begin{table}[htbp]
\centering
\caption{Scalability Analysis}
\label{tab:scalability}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Hospitals} & \textbf{Accuracy} & \textbf{Run Std.} & \textbf{Time (s)} & \textbf{Comm.~(KB)} \\
\midrule
3 & 61.5\%$\pm$0.01 & 0.020 & 0.10 & 6.9 \\
5 & 60.3\%$\pm$0.01 & 0.015 & 0.15 & 11.5 \\
7 & 61.1\%$\pm$0.01 & 0.021 & 0.22 & 16.1 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Per-Hospital Heterogeneity}

Figure~\ref{fig:client_heterogeneity} shows per-hospital performance variation under high non-IID conditions.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar,
    bar width=0.35cm,
    width=0.45\textwidth,
    height=4cm,
    ylabel={Metric Value},
    xlabel={Hospital ID},
    symbolic x coords={H1, H2, H3, H4, H5},
    xtick=data,
    ymin=0.45, ymax=0.75,
    legend style={at={(0.98,0.98)}, anchor=north east, font=\scriptsize},
    grid=major,
    grid style={dashed, gray!30},
]
\addplot[fill=blue!60] coordinates {(H1,0.64) (H2,0.61) (H3,0.58) (H4,0.63) (H5,0.64)};
\addplot[fill=green!60] coordinates {(H1,0.60) (H2,0.58) (H3,0.55) (H4,0.59) (H5,0.60)};
\addplot[fill=red!60] coordinates {(H1,0.67) (H2,0.64) (H3,0.62) (H4,0.65) (H5,0.66)};
\legend{Accuracy, F1, AUC}
\end{axis}
\end{tikzpicture}
\caption{Per-hospital performance metrics (Non-IID High).}
\label{fig:client_heterogeneity}
\end{figure}

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{Non-IID Robustness}: The framework achieves stable performance (60--61\% accuracy) across IID and non-IID configurations, with AUC-ROC consistently at 0.66. All four algorithms (FedAvg, FedProx, SCAFFOLD, FedNova) perform within 0.4pp of each other.

    \item \textbf{Privacy Cost}: Differential privacy with $\varepsilon$=10 incurs 5.2 percentage point accuracy drop; $\varepsilon$=1 (strong privacy) costs 5.8pp---acceptable for EHDS compliance while providing formal privacy guarantees.

    \item \textbf{Algorithm Equivalence}: Under moderate non-IID conditions, advanced algorithms (FedProx, SCAFFOLD, FedNova) show marginal differences from baseline FedAvg, suggesting that for EHDS healthcare analytics with careful data preprocessing, simpler algorithms suffice.

    \item \textbf{Linear Scalability}: Communication cost scales linearly with participants ($\approx$2.3 KB/hospital/round), supporting 7+ hospital federations with sub-second round times.
\end{enumerate}

% ============================================================================
% 5. EVIDENCE SYNTHESIS
% ============================================================================
\section{Evidence Synthesis}
\label{sec:evidence}

The following systematic review provides context for the technical barriers addressed by FL-EHDS and validated in Section~\ref{sec:experiments}.

\subsection{Methodology}

We conducted a systematic review following PRISMA 2020 guidelines. Database searches (PubMed, IEEE Xplore, Scopus, Web of Science, arXiv) identified 847 records; after screening, 47 documents met inclusion criteria (publication 2022-2026, explicit FL/EHDS focus, peer-reviewed or recognized institutional origin). Quality was assessed using MMAT; confidence in findings using GRADE-CERQual. Full methodology is available from the corresponding author.

\subsection{Technical Barriers}

Table~\ref{tab:barriers} summarizes FL implementation barriers with prevalence, evidence sources, and FL-EHDS mitigation strategies.

\begin{table}[htbp]
\caption{FL Implementation Barriers for EHDS}
\label{tab:barriers}
\centering
\small
\begin{tabular}{p{2.0cm}cp{2.0cm}p{1.8cm}}
\toprule
\textbf{Barrier} & \textbf{Prev.} & \textbf{Evidence} & \textbf{Mitigation} \\
\midrule
Hardware heterogeneity & 78\% & Fr\"ohlich 2025 & Adaptive engine \\
Non-IID data & 67\% & Multiple & FedProx \\
Production gap & 23\% & Fr\"ohlich 2025 & Ref. implementation \\
FHIR compliance & 34\% & Hussein 2025 & Preprocessing \\
Communication cost & High & Bonawitz 2019 & Compression \\
\bottomrule
\end{tabular}
\end{table}

\textbf{GRADE-CERQual confidence}: MODERATE for technical barriers (limited by small number of rigorous evaluations in EHDS-specific contexts).

\subsection{Legal Uncertainties}

Three critical legal questions remain unresolved, creating compliance uncertainty that inhibits organizational FL adoption~\cite{quinn2024gdpr}:

\begin{enumerate}
    \item \textbf{Gradient data status}: Are model gradients ``personal data'' under GDPR? Gradient inversion attacks demonstrate potential re-identification~\cite{zhu2019deep}, but practical feasibility in production FL remains contested.
    \item \textbf{Model anonymity thresholds}: When does an aggregated model become sufficiently ``anonymous'' to escape GDPR scope? No established legal threshold exists.
    \item \textbf{Controller/processor allocation}: In multi-party FL, who bears data controller responsibilities---data holders, aggregation server operators, or model users?
\end{enumerate}

\textbf{GRADE-CERQual confidence}: MODERATE (coherent findings but rapidly evolving regulatory landscape).

\subsection{Organizational Barriers}

HDAB capacity shows significant variation across Member States. TEHDAS assessments~\cite{tehdas2024ready} reveal Nordic countries (Estonia, Finland, Denmark) demonstrate 2-3 year advantages in HDAB capacity-building, established health data infrastructure, and cross-border experience. Southern and Eastern European states face compressed timelines with limited baseline capacity, raising concerns about implementation equity.

\textbf{GRADE-CERQual confidence}: HIGH (consistent findings across multiple high-quality studies).

% ============================================================================
% 6. IMPLEMENTATION ROADMAP
% ============================================================================
\section{Implementation Roadmap}
\label{sec:roadmap}

Table~\ref{tab:roadmap} presents a phased implementation roadmap aligned with EHDS milestones.

\begin{table}[htbp]
\caption{FL-EHDS Implementation Roadmap}
\label{tab:roadmap}
\centering
\small
\begin{tabular}{llp{3.2cm}}
\toprule
\textbf{Phase} & \textbf{Timeline} & \textbf{Priority Actions} \\
\midrule
Foundation & 2025-26 & Reference implementation; multi-MS pilots \\
Clarification & 2027 & Delegated acts; legal guidance \\
Scaling & 2028-29 & Production deployment; capacity building \\
Operation & 2029-31 & Full cross-border analytics \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Stakeholder-Specific Recommendations}

\textbf{EU Policymakers}: The March 2027 delegated acts represent a critical window. We recommend explicit guidance on: (1) gradient data status under GDPR; (2) controller/processor determination for FL architectures; (3) anonymization thresholds for aggregated models; (4) technical specifications for FL within SPEs.

\textbf{National Authorities}: Early investment in HDAB organizational capacity is essential. Staff training on FL evaluation, coordination protocols with other Member States, and stakeholder engagement with citizens about FL approaches should be prioritized. The 2-3 year Nordic advantage~\cite{tehdas2024ready} demonstrates that governance capacity may prove more constraining than technical infrastructure.

\textbf{Healthcare Organizations}: Preparation cannot wait for 2029. Organizations should: (1) accelerate FHIR compliance beyond the current 34\% baseline; (2) participate in HealthData@EU pilots to gain FL experience; (3) assess computational infrastructure for FL participation; (4) develop internal governance policies for responding to HDAB data access requests.

% ============================================================================
% 7. DISCUSSION
% ============================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Key Finding: Legal Uncertainties as Critical Blocker}

Our synthesis reveals that \textbf{legal uncertainties---not technical barriers---constitute the critical blocker} for FL adoption in EHDS contexts. While technical challenges (hardware heterogeneity, non-IID data, communication costs) are significant, they are tractable through known algorithmic solutions implemented in FL-EHDS Layer 2-3 components.

In contrast, unresolved regulatory questions create compliance uncertainty that healthcare organizations cannot navigate through engineering alone. Without clarification of gradient data status, organizations face potential GDPR violations regardless of technical privacy measures implemented. This finding aligns with van Drumpt et al.'s~\cite{vandrumpt2025pets} conclusion that governance frameworks are prerequisites, not alternatives, to technical solutions.

\subsection{Limitations}

This study has limitations informing interpretation. First, the FL/EHDS literature is rapidly evolving; publications after January 2026 are not captured. Second, most included studies analyze the newly-adopted regulation rather than actual implementation---empirical evidence on operational EHDS FL systems does not yet exist. Third, while our experimental evaluation (Section~\ref{sec:experiments}) validates framework functionality with synthetic data, real-world HealthData@EU pilot integration with clinical datasets remains essential future work. The synthetic cardiovascular data provides controlled reproducibility but cannot capture the full complexity of production EHR systems.

% ============================================================================
% 8. CONCLUSIONS
% ============================================================================
\section{Conclusions}
\label{sec:conclusions}

This paper presents FL-EHDS, a three-layer compliance framework bridging the technology-governance divide for cross-border health analytics under the European Health Data Space regulation.

Our systematic evidence synthesis reveals that \textbf{legal uncertainties---not technical barriers---constitute the critical blocker} for FL adoption in EHDS contexts. While technical challenges (hardware heterogeneity affecting 78\% of implementations, non-IID data impacting 67\% of models) are significant, they are tractable through known algorithmic solutions. The unresolved regulatory questions---gradient data status, model anonymity thresholds, controller allocation---create compliance uncertainty that discourages organizational adoption regardless of technical maturity.

The March 2027 delegated acts represent a critical window for resolution. Without explicit guidance on FL compliance, the 2029 secondary use deadline arrives with FL adoption inhibited by legal uncertainty rather than technical limitations. The 23\% production deployment rate documented in current literature~\cite{frohlich2025reality} will not improve through engineering advances alone.

\textbf{Future work} should prioritize: (1) empirical validation through HealthData@EU pilot integration; (2) citizen attitude studies examining FL acceptance and opt-out intentions; (3) economic sustainability modeling for HDAB operations; and (4) longitudinal tracking of implementation trajectories across diverse Member State contexts.

Only through coordinated action across EU policymakers, national authorities, and healthcare organizations can Federated Learning fulfill its potential as the enabling technology for privacy-preserving health analytics benefiting European citizens.

% ============================================================================
% ACKNOWLEDGMENTS
% ============================================================================
\section*{Acknowledgments}
The author thanks Prof.~Sadi Alawadi for supervision and guidance, and the TEHDAS Joint Action consortium for making preparatory materials publicly available.

% ============================================================================
% REFERENCES
% ============================================================================
\bibliographystyle{IEEEtran}

\begin{thebibliography}{00}

% === EHDS Regulation and Policy ===
\bibitem{eu2025ehds}
European Commission, ``Regulation (EU) 2025/327 on the European Health Data Space,'' \textit{Official Journal of the EU}, L 2025/327, Mar. 2025.

\bibitem{staunton2024ethical}
C. Staunton \textit{et al.}, ``Ethical and social reflections on the proposed European Health Data Space,'' \textit{Eur.~J.~Human Genetics}, vol.~32, no.~5, pp.~498--505, 2024.

\bibitem{quinn2024gdpr}
P. Quinn, E. Ellyne, and C. Yao, ``Will the GDPR restrain health data access bodies under the EHDS?'' \textit{Computer Law \& Security Review}, vol.~54, art.~105993, 2024.

\bibitem{tehdas2024ready}
TEHDAS Joint Action, ``Are EU member states ready for the European Health Data Space?'' \textit{Eur.~J.~Public Health}, vol.~34, no.~6, pp.~1102--1108, 2024.

% === EHDS Implementation Studies ===
\bibitem{frohlich2025reality}
H. Fr\"ohlich \textit{et al.}, ``Reality check: The aspirations of the EHDS amidst challenges in decentralized data analysis,'' \textit{J.~Med.~Internet Res.}, vol.~27, art.~e76491, 2025.

\bibitem{vandrumpt2025pets}
S. van Drumpt \textit{et al.}, ``Secondary use under the European Health Data Space: Setting the scene and towards a research agenda on privacy-enhancing technologies,'' \textit{Frontiers in Digital Health}, vol.~7, art.~1602101, 2025.

\bibitem{hussein2025interop}
R. Hussein \textit{et al.}, ``Interoperability framework of the EHDS for secondary use: Interactive EIF-based standards compliance toolkit,'' \textit{J.~Med.~Internet Res.}, vol.~27, art.~e69813, 2025.

\bibitem{forster2025journeys}
R. Forster \textit{et al.}, ``User journeys in cross-European secondary use of health data: Insights ahead of the EHDS,'' \textit{Eur.~J.~Public Health}, vol.~35, Suppl.~3, pp.~iii18--iii24, 2025.

\bibitem{svingel2025hdab}
L. Svingel \textit{et al.}, ``Shaping the future EHDS: Recommendations for implementation of Health Data Access Bodies,'' \textit{Eur.~J.~Public Health}, vol.~35, Suppl.~3, pp.~iii32--iii38, 2025.

\bibitem{christiansen2025pilot}
C. Christiansen \textit{et al.}, ``Piloting an infrastructure for secondary use of health data: Learnings from the HealthData@EU Pilot,'' \textit{Eur.~J.~Public Health}, vol.~35, Suppl.~3, pp.~iii3--iii4, 2025.

\bibitem{ganna2024boost}
A. Ganna, E. Ingelsson, and D. Posthuma, ``The European Health Data Space can be a boost for research beyond borders,'' \textit{Nature Medicine}, vol.~30, pp.~3053--3056, 2024.

% === Federated Learning Foundations ===
\bibitem{mcmahan2017communication}
B. McMahan \textit{et al.}, ``Communication-efficient learning of deep networks from decentralized data,'' in \textit{Proc. AISTATS}, pp.~1273--1282, 2017.

\bibitem{li2020federated}
T. Li \textit{et al.}, ``Federated optimization in heterogeneous networks,'' in \textit{Proc. MLSys}, vol.~2, pp.~429--450, 2020.

\bibitem{kairouz2021advances}
P. Kairouz \textit{et al.}, ``Advances and open problems in federated learning,'' \textit{Found.~Trends Mach.~Learn.}, vol.~14, no.~1--2, pp.~1--210, 2021.

\bibitem{rieke2020future}
N. Rieke \textit{et al.}, ``The future of digital health with federated learning,'' \textit{npj Digital Medicine}, vol.~3, art.~119, 2020.

\bibitem{bonawitz2019scale}
K. Bonawitz \textit{et al.}, ``Towards federated learning at scale: A system design,'' in \textit{Proc. MLSys}, pp.~374--388, 2019.

% === FL Systematic Reviews ===
\bibitem{teo2024systematic}
Z. L. Teo \textit{et al.}, ``Federated machine learning in healthcare: A systematic review on clinical applications and technical architecture,'' \textit{Cell Reports Medicine}, vol.~5, no.~2, art.~101419, 2024.

\bibitem{peng2024systematic}
L. Peng \textit{et al.}, ``Federated machine learning in healthcare: A systematic review on clinical applications and technical architecture,'' \textit{Comput.~Methods Programs Biomed.}, vol.~247, art.~108066, 2024.

% === Privacy and Security ===
\bibitem{zhu2019deep}
L. Zhu, Z. Liu, and S. Han, ``Deep leakage from gradients,'' in \textit{Proc. NeurIPS}, vol.~32, pp.~14774--14784, 2019.

\bibitem{shokri2017membership}
R. Shokri \textit{et al.}, ``Membership inference attacks against machine learning models,'' in \textit{Proc. IEEE S\&P}, pp.~3--18, 2017.

\bibitem{dwork2014dp}
C. Dwork and A. Roth, ``The algorithmic foundations of differential privacy,'' \textit{Found.~Trends Theor.~Comput.~Sci.}, vol.~9, no.~3--4, pp.~211--407, 2014.

\bibitem{abadi2016deep}
M. Abadi \textit{et al.}, ``Deep learning with differential privacy,'' in \textit{Proc. ACM CCS}, pp.~308--318, 2016.

% === Advanced FL Algorithms ===
\bibitem{karimireddy2020scaffold}
S. P. Karimireddy \textit{et al.}, ``SCAFFOLD: Stochastic controlled averaging for federated learning,'' in \textit{Proc. ICML}, pp.~5132--5143, 2020.

\bibitem{wang2020tackling}
J. Wang \textit{et al.}, ``Tackling the objective inconsistency problem in heterogeneous federated optimization,'' in \textit{Proc. NeurIPS}, vol.~33, pp.~7611--7623, 2020.

\bibitem{mironov2017renyi}
I. Mironov, ``R\'enyi differential privacy,'' in \textit{Proc. IEEE CSF}, pp.~263--275, 2017.

\bibitem{reddi2021adaptive}
S. Reddi \textit{et al.}, ``Adaptive federated optimization,'' in \textit{Proc. ICLR}, 2021.

% === FL Healthcare COVID-19 ===
\bibitem{dayan2021federated}
I. Dayan \textit{et al.}, ``Federated learning for predicting clinical outcomes in patients with COVID-19,'' \textit{Nature Medicine}, vol.~27, no.~10, pp.~1735--1743, 2021.

% === Gradient Privacy Attacks ===
\bibitem{carlini2022privacy}
N. Carlini \textit{et al.}, ``Membership inference attacks from first principles,'' in \textit{Proc. IEEE S\&P}, pp.~1897--1914, 2022.

% === EHDS Legal Analysis ===
\bibitem{shabani2024ehds}
M. Shabani and P. Borry, ``The European Health Data Space: Challenges and opportunities for health data governance,'' \textit{European Journal of Human Genetics}, vol.~32, no.~8, pp.~891--897, 2024.

% === Differential Privacy Healthcare ===
\bibitem{jordon2022synthetic}
J. Jordon \textit{et al.}, ``Synthetic data---A privacy mirage?'' \textit{J.~Mach.~Learn.~Res.}, vol.~23, no.~1, art.~298, 2022.

% === Clinical Imaging Datasets ===
\bibitem{chestxray2018}
D. S. Kermany \textit{et al.}, ``Identifying medical diagnoses and treatable diseases by image-based deep learning,'' \textit{Cell}, vol.~172, no.~5, pp.~1122--1131, 2018.

\end{thebibliography}

% ============================================================================
% APPENDIX - PSEUDOCODE AND SUPPLEMENTARY FIGURES (Outside page count)
% ============================================================================
\appendix

\section{FL-EHDS Algorithm Pseudocode}
\label{appendix:pseudocode}

This appendix provides formal algorithmic descriptions of the FL-EHDS framework components. Each algorithm includes detailed explanations of key steps and their relevance to EHDS compliance requirements.

\subsection{FedAvg with EHDS Compliance}

Algorithm~1 presents the core federated averaging procedure adapted for EHDS regulatory requirements. The algorithm operates in a client-server architecture where the central aggregator (typically within a Secure Processing Environment) coordinates training across distributed hospital nodes.

\textbf{Key Design Decisions:}
\begin{itemize}
    \item \textbf{ValidatePermit}: Before each training round, the HDAB-issued data permit is verified against temporal bounds and permitted purposes (EHDS Article 53). This ensures no training proceeds with expired or misaligned authorizations.
    \item \textbf{SelectParticipants}: Implements configurable client selection---full participation (default) or sampling for large federations. Selection criteria may include connectivity, historical reliability, and data freshness.
    \item \textbf{FilterOptedOut}: At each hospital, records from citizens who exercised their Article 71 opt-out rights are excluded \textit{before} any gradient computation. This filtering occurs locally to prevent opted-out data from influencing even intermediate computations.
    \item \textbf{Weighted Aggregation}: Gradients are weighted by local dataset size ($n_h$), giving larger hospitals proportionally more influence on the global model. This follows the original FedAvg formulation and is appropriate when data quality is uniform.
    \item \textbf{ClipGradient}: L2-norm clipping bounds individual hospital contributions, providing the sensitivity bound required for differential privacy and limiting the influence of any single institution.
\end{itemize}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\columnwidth}{
\small
\textbf{Algorithm 1: FL-EHDS FedAvg Training}\\[2pt]
\textbf{Input:} Hospitals $\mathcal{H} = \{h_1, \ldots, h_K\}$, permit $P$, rounds $T$\\
\textbf{Output:} Global model $\theta^{(T)}$\\[4pt]
\textbf{Server executes:}\\
\hspace*{4mm}Initialize $\theta^{(0)}$\\
\hspace*{4mm}\textbf{for} round $t = 1$ to $T$ \textbf{do}\\
\hspace*{8mm}// Governance check (Layer 1)\\
\hspace*{8mm}\textbf{if} not ValidatePermit($P$, $t$) \textbf{then abort}\\
\hspace*{8mm}$\mathcal{H}_t \leftarrow$ SelectParticipants($\mathcal{H}$)\\
\hspace*{8mm}\textbf{for each} hospital $h \in \mathcal{H}_t$ \textbf{in parallel do}\\
\hspace*{12mm}$\Delta_h^{(t)}, n_h \leftarrow$ LocalTrain($h$, $\theta^{(t-1)}$)\\
\hspace*{8mm}// Aggregation with privacy (Layer 2)\\
\hspace*{8mm}$\theta^{(t)} \leftarrow \theta^{(t-1)} + \frac{1}{\sum_h n_h} \sum_{h \in \mathcal{H}_t} n_h \cdot \Delta_h^{(t)}$\\
\hspace*{8mm}LogCompliance($t$, $\mathcal{H}_t$)\\
\hspace*{4mm}\textbf{return} $\theta^{(T)}$\\[4pt]
\textbf{LocalTrain}($h$, $\theta$) \textbf{at hospital} $h$:\\
\hspace*{4mm}// Opt-out filtering (Layer 1)\\
\hspace*{4mm}$\mathcal{D}_h \leftarrow$ FilterOptedOut($\mathcal{D}_h$, OptOutRegistry)\\
\hspace*{4mm}$\theta_h \leftarrow \theta$\\
\hspace*{4mm}\textbf{for} epoch $e = 1$ to $E$ \textbf{do}\\
\hspace*{8mm}\textbf{for} batch $\mathcal{B} \in \mathcal{D}_h$ \textbf{do}\\
\hspace*{12mm}$\theta_h \leftarrow \theta_h - \eta \nabla \mathcal{L}(\theta_h; \mathcal{B})$\\
\hspace*{4mm}$\Delta_h \leftarrow \theta_h - \theta$\\
\hspace*{4mm}// Privacy protection (Layer 3)\\
\hspace*{4mm}$\Delta_h \leftarrow$ ClipGradient($\Delta_h$, $C$)\\
\hspace*{4mm}\textbf{return} $\Delta_h$, $|\mathcal{D}_h|$
}}
\end{figure}

\subsection{Differential Privacy Mechanism}

Algorithm~2 implements the Gaussian mechanism for differential privacy, providing formal privacy guarantees through calibrated noise injection. This mechanism is applied at the aggregation server after receiving clipped gradients from hospitals.

\textbf{Mathematical Foundation:}
The noise scale $\sigma$ is computed from the Gaussian mechanism formula where $C$ is the gradient clipping threshold (sensitivity), $\varepsilon$ is the privacy parameter (smaller = stronger privacy), and $\delta$ is the failure probability (typically $10^{-5}$). The formula $\sigma = C \cdot \sqrt{2\ln(1.25/\delta)}/\varepsilon$ guarantees $(\varepsilon, \delta)$-differential privacy.

\textbf{Privacy Accountant:}
The cumulative privacy expenditure is tracked across training rounds using composition theorems. Once the total budget is exhausted, further training must cease---this hard stop prevents ``privacy bankruptcy'' where continued queries would violate the guaranteed bounds.

\textbf{Practical Considerations:}
\begin{itemize}
    \item At $\varepsilon = 10$, noise is moderate with minimal accuracy impact (1.4pp drop in our experiments).
    \item At $\varepsilon = 1$ (strong privacy), noise significantly impacts convergence (5.2pp drop).
    \item The tradeoff between $\varepsilon$ selection and model utility must be negotiated with HDABs during permit approval.
\end{itemize}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\columnwidth}{
\small
\textbf{Algorithm 2: Gaussian DP Mechanism}\\[2pt]
\textbf{Input:} Gradient $\Delta$, sensitivity $C$, privacy budget $\varepsilon$, $\delta$\\
\textbf{Output:} Noisy gradient $\tilde{\Delta}$\\[4pt]
// Compute noise scale from Gaussian mechanism\\
$\sigma \leftarrow C \cdot \sqrt{2 \ln(1.25/\delta)} / \varepsilon$\\[2pt]
// Add calibrated Gaussian noise to each parameter\\
\textbf{for each} parameter $w \in \Delta$ \textbf{do}\\
\hspace*{4mm}$\tilde{w} \leftarrow w + \mathcal{N}(0, \sigma^2)$\\[2pt]
// Track cumulative privacy expenditure\\
PrivacyAccountant.spend($\varepsilon$)\\
\textbf{if} PrivacyAccountant.budget\_exhausted() \textbf{then}\\
\hspace*{4mm}\textbf{raise} PrivacyBudgetExhaustedError\\[2pt]
\textbf{return} $\tilde{\Delta}$
}}
\end{figure}

\subsection{HDAB Permit Validation}

Algorithm~3 ensures that all FL operations comply with the data permit issued by the responsible Health Data Access Body. This validation occurs before each training round and implements the regulatory requirements of EHDS Articles 53 (permitted purposes) and Article 30 of GDPR (record-keeping).

\textbf{Validation Checks:}
\begin{itemize}
    \item \textbf{Temporal Validity}: Permits have explicit start and end dates. Continued training after expiration constitutes unauthorized processing.
    \item \textbf{Purpose Alignment}: The permit specifies allowed purposes (e.g., scientific research, AI training). Each training run is tagged with a purpose that must match permit allowances.
    \item \textbf{Category Authorization}: Different data categories (demographics, diagnoses, medications, genetic data) require separate authorization. The algorithm verifies that requested categories are covered.
    \item \textbf{Audit Logging}: Every access attempt is logged with timestamp, permit reference, categories accessed, and round number---satisfying GDPR Article 30 record-keeping requirements for regulatory inspection.
\end{itemize}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\columnwidth}{
\small
\textbf{Algorithm 3: Data Permit Validation}\\[2pt]
\textbf{Input:} Permit $P$, round $t$, requested categories $\mathcal{C}$\\
\textbf{Output:} Boolean validity\\[4pt]
// Check temporal validity (permit expiration)\\
\textbf{if} CurrentTime() $>$ $P$.valid\_until \textbf{then}\\
\hspace*{4mm}\textbf{raise} PermitExpiredError\\[2pt]
// Check purpose alignment (Article 53)\\
\textbf{if} $P$.purpose $\notin$ AllowedPurposes \textbf{then}\\
\hspace*{4mm}\textbf{raise} PurposeMismatchError\\[2pt]
// Check data category authorization\\
\textbf{for each} category $c \in \mathcal{C}$ \textbf{do}\\
\hspace*{4mm}\textbf{if} $c \notin P$.authorized\_categories \textbf{then}\\
\hspace*{8mm}\textbf{raise} UnauthorizedCategoryError\\[2pt]
// Log access for GDPR Article 30 compliance\\
AuditTrail.log(permit=$P$, round=$t$, categories=$\mathcal{C}$)\\[2pt]
\textbf{return} True
}}
\end{figure}

\subsection{Secure Aggregation Protocol}

Algorithm~4 implements secure aggregation using Shamir's secret sharing, ensuring that the aggregation server cannot observe individual hospital gradients---only their sum. This provides protection against a ``honest-but-curious'' central server.

\textbf{Protocol Phases:}
\begin{enumerate}
    \item \textbf{Secret Sharing}: Each client splits their gradient into $K$ shares using $(t, K)$-threshold Shamir secret sharing. Any $t$ shares suffice for reconstruction, but fewer reveal nothing.
    \item \textbf{Masked Aggregation}: Clients add pairwise random masks ($r_{jk}$) negotiated through key exchange. These masks are designed to cancel in the final sum.
    \item \textbf{Reconstruction}: The server collects masked gradients and computes their sum. Because $\sum_{j<k} r_{jk} - \sum_{j>k} r_{kj} = 0$ across all pairs, the masks cancel and only the true aggregate remains.
\end{enumerate}

\textbf{Security Guarantees:}
The server learns only $\Delta_{agg} = \sum_k \Delta_k$, never individual $\Delta_k$. If fewer than $t$ clients complete the round, reconstruction fails gracefully without privacy leakage.

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\columnwidth}{
\small
\textbf{Algorithm 4: Secure Aggregation}\\[2pt]
\textbf{Input:} Client gradients $\{\Delta_1, \ldots, \Delta_K\}$, threshold $t$\\
\textbf{Output:} Aggregated gradient $\Delta_{agg}$\\[4pt]
// Phase 1: Shamir secret sharing\\
\textbf{for each} client $k$ \textbf{do}\\
\hspace*{4mm}shares$_k \leftarrow$ ShamirShare($\Delta_k$, $t$, $K$)\\
\hspace*{4mm}Distribute shares$_k$ to other clients\\[2pt]
// Phase 2: Add pairwise random masks\\
\textbf{for each} client $k$ \textbf{do}\\
\hspace*{4mm}$\hat{\Delta}_k \leftarrow \Delta_k + \sum_{j<k} r_{jk} - \sum_{j>k} r_{kj}$\\[2pt]
// Phase 3: Server reconstructs aggregate\\
$\Delta_{agg} \leftarrow \sum_{k=1}^{K} \hat{\Delta}_k$\\
// Masks cancel: $\sum_k \sum_{j<k} r_{jk} - \sum_k \sum_{j>k} r_{kj} = 0$\\[2pt]
\textbf{if} ActiveClients $< t$ \textbf{then}\\
\hspace*{4mm}\textbf{raise} SecureAggregationError\\[2pt]
\textbf{return} $\Delta_{agg}$
}}
\end{figure}

\subsection{FedProx for Non-IID Data}

Algorithm~5 extends FedAvg to handle heterogeneous (non-IID) data distributions common in cross-border healthcare settings. The proximal term $\mu$ regularizes local updates toward the global model, preventing drift when hospitals have skewed patient populations.

\textbf{Intuition:}
In standard FedAvg, hospitals with extreme data distributions may compute gradients that diverge significantly from the global optimum. FedProx adds a penalty term $\frac{\mu}{2}\|\theta_h - \theta\|^2$ to the local objective, ensuring local models remain ``close'' to the global model.

\textbf{Parameter Selection:}
\begin{itemize}
    \item $\mu = 0$: Equivalent to FedAvg (no regularization).
    \item $\mu = 0.01$--$0.1$: Moderate regularization; our experiments show stable convergence with minimal accuracy impact.
    \item $\mu > 1$: Strong regularization; may prevent adaptation to local data characteristics.
\end{itemize}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\columnwidth}{
\small
\textbf{Algorithm 5: FedProx Local Update}\\[2pt]
\textbf{Input:} Local data $\mathcal{D}_h$, global model $\theta$, proximal weight $\mu$\\
\textbf{Output:} Local update $\Delta_h$\\[4pt]
// Initialize local model from global\\
$\theta_h \leftarrow \theta$\\[2pt]
// Local training with proximal regularization\\
\textbf{for} epoch $e = 1$ to $E$ \textbf{do}\\
\hspace*{4mm}\textbf{for} batch $\mathcal{B} \in \mathcal{D}_h$ \textbf{do}\\
\hspace*{8mm}// Standard loss gradient\\
\hspace*{8mm}$g \leftarrow \nabla \mathcal{L}(\theta_h; \mathcal{B})$\\
\hspace*{8mm}// Add proximal term gradient: $\nabla \frac{\mu}{2}\|\theta_h - \theta\|^2$\\
\hspace*{8mm}$g \leftarrow g + \mu(\theta_h - \theta)$\\
\hspace*{8mm}// Update local model\\
\hspace*{8mm}$\theta_h \leftarrow \theta_h - \eta \cdot g$\\[2pt]
// Compute update delta\\
$\Delta_h \leftarrow \theta_h - \theta$\\[2pt]
\textbf{return} $\Delta_h$
}}
\end{figure}

\subsection{Article 71 Opt-Out Registry Protocol}

Algorithm~6 implements the EHDS Article 71 opt-out mechanism, enabling citizens to withdraw their electronic health data from secondary use. The protocol ensures that opted-out records are excluded from FL training while maintaining computational efficiency.

\textbf{Design Considerations:}
\begin{itemize}
    \item \textbf{Real-time vs. Batch}: Full registry synchronization before each round ensures compliance but incurs latency. Cached mode with periodic refresh balances compliance and performance.
    \item \textbf{Granularity}: Opt-out may apply to all secondary use, specific purposes, or specific categories. The algorithm supports fine-grained filtering.
    \item \textbf{Auditability}: Every filtering operation is logged, enabling demonstration of compliance during regulatory audits.
\end{itemize}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\columnwidth}{
\small
\textbf{Algorithm 6: Article 71 Opt-Out Filtering}\\[2pt]
\textbf{Input:} Local dataset $\mathcal{D}_h$, purpose $p$, categories $\mathcal{C}$\\
\textbf{Output:} Filtered dataset $\mathcal{D}'_h$\\[4pt]
// Synchronize with national opt-out registry\\
OptOutRecords $\leftarrow$ FetchOptOutRegistry(MemberState)\\[2pt]
// Initialize filtered dataset\\
$\mathcal{D}'_h \leftarrow \emptyset$\\[2pt]
\textbf{for each} record $r \in \mathcal{D}_h$ \textbf{do}\\
\hspace*{4mm}citizen\_id $\leftarrow$ r.pseudonymized\_id\\
\hspace*{4mm}opted\_out $\leftarrow$ False\\[2pt]
\hspace*{4mm}// Check purpose-specific opt-out\\
\hspace*{4mm}\textbf{if} (citizen\_id, $p$) $\in$ OptOutRecords \textbf{then}\\
\hspace*{8mm}opted\_out $\leftarrow$ True\\[2pt]
\hspace*{4mm}// Check category-specific opt-out\\
\hspace*{4mm}\textbf{for each} $c \in \mathcal{C}$ \textbf{do}\\
\hspace*{8mm}\textbf{if} (citizen\_id, $c$) $\in$ OptOutRecords \textbf{then}\\
\hspace*{12mm}opted\_out $\leftarrow$ True\\[2pt]
\hspace*{4mm}\textbf{if} not opted\_out \textbf{then}\\
\hspace*{8mm}$\mathcal{D}'_h \leftarrow \mathcal{D}'_h \cup \{r\}$\\[2pt]
// Log filtering statistics for audit\\
AuditLog.record(total=$|\mathcal{D}_h|$, filtered=$|\mathcal{D}'_h|$)\\[2pt]
\textbf{return} $\mathcal{D}'_h$
}}
\end{figure}

\subsection{FHIR R4 Preprocessing Pipeline}

Algorithm~7 standardizes heterogeneous EHR data into the FHIR R4 format required for interoperable FL training. Given that only 34\% of European healthcare providers achieve full FHIR compliance, this preprocessing step is essential for practical deployment.

\textbf{Pipeline Stages:}
\begin{enumerate}
    \item \textbf{Format Detection}: Identifies source format (HL7 v2, CDA, proprietary CSV, etc.) using heuristic signatures.
    \item \textbf{Terminology Mapping}: Converts local coding systems to standard terminologies (ICD-10, SNOMED-CT, LOINC) using UMLS mappings.
    \item \textbf{FHIR Transformation}: Constructs FHIR resources (Patient, Observation, Condition, MedicationStatement) from normalized data.
    \item \textbf{Tensor Conversion}: Extracts numerical features from FHIR resources into tensors suitable for ML training.
\end{enumerate}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\columnwidth}{
\small
\textbf{Algorithm 7: FHIR R4 Preprocessing}\\[2pt]
\textbf{Input:} Raw EHR records $\mathcal{R}$, feature specification $\mathcal{F}$\\
\textbf{Output:} Training tensors $(X, y)$\\[4pt]
// Detect source format and select parser\\
format $\leftarrow$ DetectFormat($\mathcal{R}$)\\
parser $\leftarrow$ GetParser(format)\\[2pt]
// Parse to intermediate representation\\
records $\leftarrow$ parser.parse($\mathcal{R}$)\\[2pt]
// Map local codes to standard terminologies\\
\textbf{for each} $r \in$ records \textbf{do}\\
\hspace*{4mm}$r$.diagnoses $\leftarrow$ MapToICD10($r$.diagnoses)\\
\hspace*{4mm}$r$.medications $\leftarrow$ MapToATC($r$.medications)\\
\hspace*{4mm}$r$.labs $\leftarrow$ MapToLOINC($r$.labs)\\[2pt]
// Convert to FHIR R4 resources\\
fhir\_bundle $\leftarrow$ ToFHIR(records)\\
ValidateFHIR(fhir\_bundle)\\[2pt]
// Extract features into tensors\\
$X \leftarrow$ ExtractFeatures(fhir\_bundle, $\mathcal{F}$)\\
$y \leftarrow$ ExtractLabels(fhir\_bundle)\\[2pt]
// Normalize numerical features\\
$X \leftarrow$ StandardScaler.fit\_transform($X$)\\[2pt]
\textbf{return} $(X, y)$
}}
\end{figure}

\subsection{Privacy Budget Accountant}

Algorithm~8 tracks cumulative privacy expenditure across FL training rounds using moment accountant composition. This enables tight privacy bounds when training for many rounds while ensuring the total guarantee is never exceeded.

\textbf{Technical Details:}
The moment accountant (R\'enyi DP) provides tighter composition bounds than basic composition. For $T$ rounds with per-round privacy cost $(\varepsilon_t, \delta_t)$, the total privacy loss is computed via the log-moment generating function, enabling longer training within the same budget.

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\columnwidth}{
\small
\textbf{Algorithm 8: Privacy Budget Accountant}\\[2pt]
\textbf{Input:} Total budget $(\varepsilon_{total}, \delta_{total})$, rounds $T$\\
\textbf{Output:} Per-round budget allocation\\[4pt]
// Initialize moment accountant state\\
$\lambda \leftarrow$ [0] $\times$ MAX\_ORDER \hfill // R\'enyi moments\\
rounds\_completed $\leftarrow$ 0\\[2pt]
\textbf{function} AllocateRound():\\
\hspace*{4mm}// Compute remaining budget\\
\hspace*{4mm}$\varepsilon_{spent} \leftarrow$ ComputeEpsilon($\lambda$, $\delta_{total}$)\\
\hspace*{4mm}$\varepsilon_{remaining} \leftarrow \varepsilon_{total} - \varepsilon_{spent}$\\[2pt]
\hspace*{4mm}// Check if budget allows another round\\
\hspace*{4mm}\textbf{if} $\varepsilon_{remaining} < \varepsilon_{min}$ \textbf{then}\\
\hspace*{8mm}\textbf{raise} BudgetExhaustedError\\[2pt]
\hspace*{4mm}// Allocate per-round budget\\
\hspace*{4mm}$\varepsilon_t \leftarrow \varepsilon_{remaining} / (T -$ rounds\_completed$)$\\
\hspace*{4mm}\textbf{return} $\varepsilon_t$\\[2pt]
\textbf{function} RecordRound($\sigma$, $q$):\\
\hspace*{4mm}// Update moments after each round\\
\hspace*{4mm}\textbf{for} order $= 1$ to MAX\_ORDER \textbf{do}\\
\hspace*{8mm}$\lambda$[order] $+= $ ComputeMoment(order, $\sigma$, $q$)\\
\hspace*{4mm}rounds\_completed $+= 1$
}}
\end{figure}

% ============================================================================
% APPENDIX B - SUPPLEMENTARY EXPERIMENTAL FIGURES
% ============================================================================
\section{Supplementary Experimental Figures}
\label{appendix:figures}

This section presents detailed experimental results from the FL-EHDS benchmark suite, providing insights into client heterogeneity, training dynamics, and system performance. All figures are generated from real experimental runs available in the repository.

\subsection{Hospital Data Distribution}

Figure~\ref{fig:data_distribution} illustrates the non-IID nature of data across the five simulated hospitals. Each hospital exhibits distinct demographic characteristics reflecting real-world geographical variation in European patient populations.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/figA1_data_distribution.pdf}
\caption{Data distribution across hospitals. Metrics shown: sample count, mean age, BMI, systolic BP, glucose, and positive class rate. Notable heterogeneity: Amsterdam shows older population (60.8 years mean age) with higher positive rate (62.6\%) compared to Rome (49.1 years, 39.2\%).}
\label{fig:data_distribution}
\end{figure}

\subsection{Per-Client Training Time}

Figure~\ref{fig:training_times} shows training time variation across clients per round. Differences arise from local dataset sizes (300--500 records), hardware capabilities, and network conditions.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/figA2_training_times.pdf}
\caption{Per-client training time per round. Larger hospitals (Berlin: 500 samples) exhibit slightly longer training times. The adaptive training engine compensates by adjusting batch sizes for stragglers.}
\label{fig:training_times}
\end{figure}

\subsection{Client Participation Matrix}

Figure~\ref{fig:participation} presents the client participation matrix over 50 training rounds. Not all clients participate in every round due to availability, connectivity, or straggler timeout policies.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/figA3_participation_matrix.pdf}
\caption{Client participation matrix (50 rounds $\times$ 5 clients). Participation rates: IT 88\%, DE 86\%, FR 86\%, ES 88\%, NL 92\%. The framework tolerates 10--15\% dropout per round while maintaining convergence.}
\label{fig:participation}
\end{figure}

\subsection{Gradient Norm Evolution}

Figure~\ref{fig:gradient_norms} tracks gradient L2-norms throughout training. Decreasing gradient norms indicate model convergence; divergent norms suggest instability.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/figA4_gradient_norms.pdf}
\caption{Gradient norm evolution per client over 50 rounds. All clients show decreasing trends indicating stable convergence. Clipping threshold $C=1.0$ bounds extreme values for DP compatibility.}
\label{fig:gradient_norms}
\end{figure}

\subsection{Communication Cost Analysis}

Figure~\ref{fig:communication} analyzes per-round communication overhead. For logistic regression with 6 parameters, each gradient transmission is approximately 2.3 KB (32-bit floats + protocol overhead).

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/figA5_communication_cost.pdf}
\caption{Cumulative communication cost per round. Linear scaling with participating clients (3.5 KB/client/round). Total 50-round overhead: 875 KB for 5 clients---feasible even for bandwidth-constrained environments.}
\label{fig:communication}
\end{figure}

\subsection{Learning Rate Sensitivity}

Figure~\ref{fig:learning_rate} compares convergence across learning rates $\eta \in \{0.01, 0.05, 0.1, 0.2, 0.5\}$. Optimal performance is achieved at $\eta = 0.1$--$0.2$.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/figA6_learning_rate.pdf}
\caption{Learning rate sensitivity analysis. $\eta = 0.01$: slow convergence (53.8\% at round 50). $\eta = 0.1$: optimal (58.6\%). $\eta = 0.5$: instability with oscillations.}
\label{fig:learning_rate}
\end{figure}

\subsection{Batch Size Impact}

Figure~\ref{fig:batch_size} evaluates the effect of batch sizes $\{8, 16, 32, 64, 128\}$ on convergence speed and final accuracy.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/figA7_batch_size.pdf}
\caption{Batch size impact on convergence. Smaller batches (8--16) provide noisier gradients but faster initial progress. Batch size 32 balances gradient quality and computational efficiency.}
\label{fig:batch_size}
\end{figure}

\subsection{Per-Client Accuracy Trajectories}

Figure~\ref{fig:client_accuracy} shows individual client accuracy trajectories, revealing heterogeneity in local model performance.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/figA8_client_accuracy.pdf}
\caption{Per-client accuracy over training rounds. Variance reflects non-IID data: NL (older, higher-risk population) reaches 64\% accuracy while FR (mid-range demographics) stabilizes at 55\%.}
\label{fig:client_accuracy}
\end{figure}

% ============================================================================
% APPENDIX C - FL ALGORITHM COMPARISON
% ============================================================================
\section{FL Algorithm Comparison Analysis}
\label{appendix:algorithm_comparison}

This appendix provides comprehensive benchmarking of five FL algorithms across varying degrees of data heterogeneity. Results inform algorithm selection for different EHDS deployment scenarios.

\subsection{Algorithms Evaluated}

We compare five prominent FL algorithms:

\begin{itemize}
    \item \textbf{FedAvg}~\cite{mcmahan2017communication}: Baseline federated averaging with weighted aggregation.
    \item \textbf{FedProx}~\cite{li2020federated}: Proximal term ($\mu$) regularizes local drift in non-IID settings.
    \item \textbf{SCAFFOLD}~\cite{karimireddy2020scaffold}: Variance reduction through control variates correcting client drift.
    \item \textbf{FedAdam}: Adaptive server-side optimization with momentum and second-moment estimates.
    \item \textbf{FedYogi}~\cite{reddi2021adaptive}: Variant of FedAdam with improved stability for sparse gradients.
\end{itemize}

\subsection{Non-IID Configuration}

Data heterogeneity is controlled via Dirichlet distribution with concentration parameter $\alpha$:
\begin{itemize}
    \item $\alpha = 0.1$: \textbf{Extreme non-IID} --- highly skewed label distributions per client
    \item $\alpha = 0.5$: \textbf{High non-IID} --- significant heterogeneity
    \item $\alpha = 1.0$: \textbf{Moderate non-IID} --- balanced heterogeneity
    \item $\alpha = 10.0$: \textbf{Near-IID} --- approximately uniform distributions
\end{itemize}

\subsection{Convergence at Different Heterogeneity Levels}

Figure~\ref{fig:algo_comparison_noniid} shows convergence trajectories for all algorithms across four non-IID levels. Key observations:

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/fig_algorithm_comparison_noniid.pdf}
\caption{Algorithm convergence across non-IID levels ($\alpha \in \{0.1, 0.5, 1.0, 10.0\}$). SCAFFOLD and adaptive methods (FedAdam, FedYogi) show superior stability under extreme heterogeneity ($\alpha=0.1$).}
\label{fig:algo_comparison_noniid}
\end{figure}

\textbf{Findings:}
\begin{enumerate}
    \item At $\alpha=0.1$ (extreme non-IID), \textbf{SCAFFOLD} achieves most stable convergence due to variance reduction.
    \item \textbf{FedProx} provides marginal improvement over FedAvg at moderate heterogeneity ($\alpha=0.5$--$1.0$).
    \item \textbf{Adaptive methods} (FedAdam, FedYogi) excel in near-IID settings ($\alpha=10$) but may oscillate under extreme heterogeneity.
    \item \textbf{FedAvg} remains competitive in near-IID conditions, making it suitable for homogeneous federations.
\end{enumerate}

\subsection{Final Accuracy vs. Data Heterogeneity}

Figure~\ref{fig:accuracy_vs_noniid} summarizes final accuracy (round 50) as a function of heterogeneity level.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/fig_accuracy_vs_noniid.pdf}
\caption{Final accuracy vs. Dirichlet $\alpha$. All algorithms degrade under extreme non-IID conditions. SCAFFOLD shows smallest performance gap between $\alpha=0.1$ and $\alpha=10$.}
\label{fig:accuracy_vs_noniid}
\end{figure}

\textbf{EHDS Implications:}
Cross-border federations with heterogeneous patient populations (different age distributions, disease prevalence, clinical practices) should prefer SCAFFOLD or FedProx over baseline FedAvg.

\subsection{Convergence Speed Analysis}

Figure~\ref{fig:convergence_speed} analyzes two convergence metrics: (a) rounds required to reach 55\% accuracy threshold, and (b) best accuracy achieved within the first 20 rounds.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/fig_convergence_speed.pdf}
\caption{Convergence speed comparison. Left: rounds to 55\% accuracy. Right: best accuracy in first 20 rounds. Adaptive methods converge faster in early rounds but may plateau.}
\label{fig:convergence_speed}
\end{figure}

\textbf{Resource-Constrained Scenarios:}
For EHDS deployments with limited communication budgets or time constraints, adaptive methods (FedAdam, FedYogi) may be preferred for their faster early convergence, despite potential instability in later rounds.

\subsection{Algorithm Selection Guidelines for EHDS}

Table~\ref{tab:algo_selection} provides recommendations based on federation characteristics:

\begin{table}[htbp]
\centering
\caption{Algorithm Selection Guidelines for EHDS Deployments}
\label{tab:algo_selection}
\small
\begin{tabular}{p{2.8cm}p{2.2cm}p{2.5cm}}
\toprule
\textbf{Scenario} & \textbf{Recommended} & \textbf{Rationale} \\
\midrule
Homogeneous MS & FedAvg & Simplicity, proven \\
Heterogeneous MS & SCAFFOLD & Variance reduction \\
Resource-limited & FedAdam & Fast convergence \\
Privacy-critical & FedAvg + DP & Well-studied DP bounds \\
Sparse participation & FedProx & Dropout resilience \\
\bottomrule
\end{tabular}
\end{table}

\textbf{MS} = Member States. Heterogeneity arises from demographic differences, clinical coding practices, EHR system variations, and population health profiles across EU regions.

\subsection{Reproducibility}

All algorithm comparison results are reproducible via:
\begin{verbatim}
cd fl-ehds-framework
python benchmarks/algorithm_comparison.py
\end{verbatim}

\noindent Figures are generated in \texttt{./results\_algorithms/}.

% ============================================================================
% APPENDIX D - ADVANCED FL PARADIGMS
% ============================================================================
\section{Advanced Federated Learning Paradigms}
\label{appendix:advanced_fl}

The FL-EHDS framework implements several advanced FL paradigms addressing specific challenges in cross-border healthcare analytics. This appendix documents these components available in the reference implementation.

\subsection{Vertical Federated Learning}

Vertical FL (VFL) addresses scenarios where different institutions hold different features for the same patients---common in healthcare where hospitals, labs, and pharmacies maintain complementary records.

\textbf{Private Set Intersection (PSI):} Before training, participating institutions must identify common patients without revealing their full patient lists. The framework implements RSA-based PSI with the following properties:
\begin{itemize}
    \item \textbf{Privacy}: Neither party learns patients not in the intersection
    \item \textbf{Efficiency}: $O(n \log n)$ complexity for $n$ records
    \item \textbf{EHDS Compliance}: Pseudonymized identifiers only
\end{itemize}

\textbf{Split Learning:} Neural networks are partitioned across institutions. Each party computes forward passes on their features up to a ``cut layer,'' exchanges activations (not raw data), and backpropagates gradients. The framework supports:
\begin{itemize}
    \item U-shaped splits (features split, labels at one party)
    \item Configurable cut layer selection
    \item Gradient compression for bandwidth optimization
\end{itemize}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\columnwidth}{
\small
\textbf{Algorithm D.1: Split Learning Forward Pass}\\[2pt]
\textbf{Input:} Features $X_A$, $X_B$ at parties A, B; cut layer $k$\\
\textbf{Output:} Prediction $\hat{y}$\\[4pt]
// Party A: forward to cut layer\\
$h_A \leftarrow f_{1:k}^A(X_A)$ \hfill // Local features  activations\\[2pt]
// Party B: forward to cut layer\\
$h_B \leftarrow f_{1:k}^B(X_B)$ \hfill // Local features  activations\\[2pt]
// Server: aggregate and complete forward pass\\
$h \leftarrow \text{Concat}(h_A, h_B)$\\
$\hat{y} \leftarrow f_{k+1:L}(h)$ \hfill // Cut layer  output\\[2pt]
\textbf{return} $\hat{y}$
}}
\end{figure}

\subsection{Byzantine-Resilient Aggregation}

In cross-border federations, some participants may be compromised, malfunctioning, or malicious. Byzantine-resilient aggregation protects model integrity against up to $f < n/3$ adversarial clients.

\textbf{Implemented Defenses:}
\begin{itemize}
    \item \textbf{Krum}: Selects the gradient closest to its $n-f-2$ nearest neighbors, excluding outliers
    \item \textbf{Trimmed Mean}: Discards the $\beta$ fraction of extreme values per coordinate before averaging
    \item \textbf{Coordinate-wise Median}: Robust estimator immune to single-coordinate attacks
    \item \textbf{Bulyan}: Two-stage defense combining Krum selection with trimmed mean
    \item \textbf{FLTrust}: Server maintains a small trusted dataset to validate client updates
\end{itemize}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\columnwidth}{
\small
\textbf{Algorithm D.2: Krum Byzantine Defense}\\[2pt]
\textbf{Input:} Gradients $\{g_1, \ldots, g_n\}$, Byzantine bound $f$\\
\textbf{Output:} Selected gradient $g^*$\\[4pt]
\textbf{for each} gradient $g_i$ \textbf{do}\\
\hspace*{4mm}// Compute distances to all other gradients\\
\hspace*{4mm}$D_i \leftarrow \{\|g_i - g_j\|^2 : j \neq i\}$\\
\hspace*{4mm}// Sum of $n-f-2$ smallest distances\\
\hspace*{4mm}$s_i \leftarrow \sum_{d \in \text{smallest}_{n-f-2}(D_i)} d$\\[2pt]
// Select gradient with minimum score\\
$g^* \leftarrow g_{\arg\min_i s_i}$\\[2pt]
\textbf{return} $g^*$
}}
\end{figure}

\subsection{Continual Federated Learning}

Healthcare data evolves over time---new diseases emerge, treatment protocols change, and patient demographics shift. Continual FL addresses catastrophic forgetting while adapting to distribution drift.

\textbf{Implemented Strategies:}
\begin{itemize}
    \item \textbf{Elastic Weight Consolidation (EWC)}: Protects important weights using Fisher Information as importance measure
    \item \textbf{Learning without Forgetting (LwF)}: Knowledge distillation from previous model prevents forgetting
    \item \textbf{Experience Replay}: Maintains memory buffer of representative past samples
    \item \textbf{Drift Detection}: Statistical tests (ADWIN, Page-Hinkley) detect concept drift and trigger adaptation
\end{itemize}

The EWC loss function adds a quadratic penalty for deviating from important parameters:
\[
\mathcal{L}_{EWC}(\theta) = \mathcal{L}(\theta) + \frac{\lambda}{2} \sum_i F_i (\theta_i - \theta_i^*)^2
\]
where $F_i$ is the Fisher Information for parameter $i$ and $\theta^*$ are the optimal parameters for previous tasks.

\subsection{Multi-Task Federated Learning}

Different hospitals may have different prediction objectives (e.g., mortality, readmission, length-of-stay). Multi-Task FL learns shared representations while allowing task-specific outputs.

\textbf{Architectures:}
\begin{itemize}
    \item \textbf{Hard Parameter Sharing}: Common feature extractor, task-specific heads
    \item \textbf{Soft Parameter Sharing}: Separate networks with regularization encouraging similarity
    \item \textbf{FedMTL}: Learns task relationships dynamically during training
\end{itemize}

\subsection{Hierarchical Federated Learning}

The FL-EHDS framework implements a four-tier hierarchy reflecting EU governance structure:

\begin{enumerate}
    \item \textbf{Client Tier}: Individual hospitals/data holders
    \item \textbf{Regional Tier}: Regional aggregators (e.g., Lombardy, Bavaria)
    \item \textbf{National Tier}: National HDABs coordinate Member State aggregation
    \item \textbf{EU Tier}: HealthData@EU central aggregator
\end{enumerate}

This hierarchy reduces communication costs (hospitals communicate with regional aggregators, not directly with EU server) and aligns with EHDS governance where HDABs have national jurisdiction.

\subsection{Personalized Federated Learning}

Standard FL produces a single global model, but healthcare institutions have distinct patient populations requiring personalized predictions. Personalized FL learns models adapted to each client's data distribution while benefiting from collaborative training.

\textbf{Implemented Approaches:}
\begin{itemize}
    \item \textbf{FedPer (Federated Personalization)}: Shares base layers globally, keeps personalization layers local. Suitable when institutions have similar low-level features but different high-level patterns.
    \item \textbf{pFedMe (Personalized FedAvg with Moreau Envelopes)}: Uses Moreau envelopes to decouple personalization from global model learning. Provides theoretical convergence guarantees.
    \item \textbf{Per-FedAvg (Personalized FedAvg)}: Meta-learning approach where the global model serves as initialization for local fine-tuning. Based on MAML (Model-Agnostic Meta-Learning).
    \item \textbf{APFL (Adaptive Personalized FL)}: Learns mixing coefficient $\alpha$ between global and local models per client. Automatically balances personalization vs.\ generalization.
    \item \textbf{Ditto}: Adds personalization regularization term to local objectives, ensuring local models remain close to global while adapting to local data.
\end{itemize}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\columnwidth}{
\small
\textbf{Algorithm D.3: pFedMe Local Update}\\[2pt]
\textbf{Input:} Local data $\mathcal{D}_k$, global model $\theta$, personal model $\theta_k$, $\lambda$, $\eta$\\
\textbf{Output:} Updated personal model $\theta_k'$\\[4pt]
// Personalization step: optimize local objective\\
\textbf{for} $i = 1$ to $R$ \textbf{do}\\
\hspace*{4mm}$\theta_k \leftarrow \theta_k - \eta \nabla \mathcal{L}(\theta_k; \mathcal{D}_k)$\\[2pt]
// Moreau envelope update: balance with global\\
$\theta_k' \leftarrow \theta_k - \lambda(\theta_k - \theta)$\\[2pt]
// Compute gradient for global model\\
$g_k \leftarrow \lambda(\theta - \theta_k')$\\[2pt]
\textbf{return} $\theta_k'$, $g_k$
}}
\end{figure}

\textbf{EHDS Relevance:} Personalized FL is critical for EHDS because Member States have different healthcare systems, disease prevalence, and clinical practices. A model trained for Finnish cardiovascular risk may not transfer directly to Italian populations due to dietary, genetic, and healthcare access differences.

\subsection{Asynchronous Federated Learning}

Synchronous FL requires all clients to complete local training before aggregation, creating bottlenecks when clients have heterogeneous computational resources or network connectivity. Asynchronous FL allows clients to contribute updates independently.

\textbf{Implemented Strategies:}
\begin{itemize}
    \item \textbf{FedAsync}: Server aggregates each client update immediately upon arrival. Uses staleness-weighted averaging to discount outdated gradients.
    \item \textbf{FedBuff (Buffered Asynchronous)}: Collects $K$ updates in a buffer before aggregating. Balances asynchrony benefits with aggregation quality.
    \item \textbf{ASO-Fed (Adaptive Synchronization)}: Dynamically adjusts synchronization frequency based on client staleness distribution.
    \item \textbf{Semi-Asynchronous}: Waits for a fraction $\alpha$ of clients (e.g., 70\%) before aggregating, bounding maximum staleness.
\end{itemize}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\columnwidth}{
\small
\textbf{Algorithm D.4: FedAsync with Staleness Weighting}\\[2pt]
\textbf{Input:} Client update $\Delta_k$, client round $t_k$, server round $t$\\
\textbf{Output:} Updated global model $\theta$\\[4pt]
// Compute staleness\\
$\tau \leftarrow t - t_k$ \hfill // How many rounds old is this update?\\[2pt]
// Staleness-weighted coefficient (polynomial decay)\\
$\alpha \leftarrow (1 + \tau)^{-a}$ \hfill // $a > 0$ controls decay rate\\[2pt]
// Apply weighted update\\
$\theta \leftarrow \theta + \alpha \cdot \eta \cdot \Delta_k$\\[2pt]
\textbf{return} $\theta$
}}
\end{figure}

\textbf{Staleness Functions:}
\begin{itemize}
    \item \textbf{Constant}: $\alpha(\tau) = 1$ (ignore staleness)
    \item \textbf{Polynomial}: $\alpha(\tau) = (1+\tau)^{-a}$
    \item \textbf{Exponential}: $\alpha(\tau) = e^{-a\tau}$
    \item \textbf{Hinge}: $\alpha(\tau) = 1$ if $\tau \leq \tau_{max}$, else 0
\end{itemize}

\textbf{EHDS Relevance:} Cross-border federations span institutions with vastly different IT infrastructure. A Finnish hospital with modern HPC may complete training in minutes, while a rural clinic in another Member State may require hours. Asynchronous FL prevents fast clients from waiting for stragglers.

\subsection{Fairness-Aware Federated Learning}

FL can amplify existing healthcare disparities if minority populations or under-resourced institutions are underrepresented in training. Fairness-aware FL ensures equitable model performance across demographic groups and participating institutions.

\textbf{Fairness Dimensions:}
\begin{itemize}
    \item \textbf{Client Fairness}: Equitable accuracy across participating hospitals
    \item \textbf{Group Fairness}: Balanced performance across demographic groups (age, gender, ethnicity)
    \item \textbf{Individual Fairness}: Similar predictions for similar patients
\end{itemize}

\textbf{Implemented Approaches:}
\begin{itemize}
    \item \textbf{q-FedAvg}: Reweights client contributions to minimize worst-case client loss. Parameter $q$ controls fairness-accuracy tradeoff.
    \item \textbf{AFL (Agnostic Federated Learning)}: Optimizes for the worst-performing distribution mixture, providing robustness guarantees.
    \item \textbf{FedMGDA+ (Multi-Gradient Descent)}: Finds Pareto-optimal update direction balancing all client objectives.
    \item \textbf{TERM (Tilted Empirical Risk Minimization)}: Uses tilted losses to emphasize underperforming groups.
    \item \textbf{FairFed}: Adds fairness constraints (demographic parity, equalized odds) to the federated optimization.
\end{itemize}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\columnwidth}{
\small
\textbf{Algorithm D.5: q-FedAvg Fair Aggregation}\\[2pt]
\textbf{Input:} Client losses $\{L_1, \ldots, L_K\}$, updates $\{\Delta_1, \ldots, \Delta_K\}$, $q$\\
\textbf{Output:} Fair aggregated update $\Delta$\\[4pt]
// Compute fairness weights (emphasize high-loss clients)\\
\textbf{for each} client $k$ \textbf{do}\\
\hspace*{4mm}$w_k \leftarrow L_k^q$ \hfill // Higher loss  higher weight\\[2pt]
// Normalize weights\\
$W \leftarrow \sum_k w_k$\\
\textbf{for each} client $k$ \textbf{do}\\
\hspace*{4mm}$w_k \leftarrow w_k / W$\\[2pt]
// Weighted aggregation\\
$\Delta \leftarrow \sum_k w_k \cdot \Delta_k$\\[2pt]
\textbf{return} $\Delta$
}}
\end{figure}

\textbf{Fairness Metrics:}
\begin{itemize}
    \item \textbf{Performance Variance}: $\text{Var}(\{L_k\}_{k=1}^K)$ across clients
    \item \textbf{Worst-case Loss}: $\max_k L_k$
    \item \textbf{Demographic Parity Gap}: $|P(\hat{Y}=1|A=0) - P(\hat{Y}=1|A=1)|$
    \item \textbf{Equalized Odds Gap}: Difference in TPR/FPR across groups
\end{itemize}

\textbf{EHDS Relevance:} The EHDS explicitly aims to benefit all European citizens. Models that perform well on average but poorly for specific populations (e.g., rare diseases, elderly patients, specific ethnic groups) violate the regulation's equity principles. Fairness-aware FL ensures no Member State or demographic group is systematically disadvantaged.

% ============================================================================
% APPENDIX E - INFRASTRUCTURE COMPONENTS
% ============================================================================
\section{Infrastructure Components}
\label{appendix:infrastructure}

Production deployment of FL-EHDS requires enterprise-grade infrastructure. This appendix documents the infrastructure modules available in the reference implementation.

\subsection{Communication Layer}

The framework provides two transport options optimized for different deployment scenarios:

\textbf{gRPC (Google Remote Procedure Call):}
\begin{itemize}
    \item Bidirectional streaming for continuous gradient exchange
    \item Protocol Buffers serialization (30\% bandwidth reduction vs.\ JSON)
    \item HTTP/2 multiplexing reduces connection overhead
    \item Ideal for: data center deployments, low-latency requirements
\end{itemize}

\textbf{WebSocket:}
\begin{itemize}
    \item Browser-compatible for web-based FL clients
    \item Firewall-friendly (standard HTTP upgrade)
    \item Event-driven architecture for real-time updates
    \item Ideal for: edge deployments, browser-based participation
\end{itemize}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\columnwidth}{
\small
\textbf{Communication Manager Configuration}\\[2pt]
\texttt{transport: gRPC | WebSocket}\\
\texttt{compression: gzip | lz4 | zstd | none}\\
\texttt{chunk\_size: 1MB} \hfill // For streaming large models\\
\texttt{retry\_policy:}\\
\hspace*{4mm}\texttt{max\_retries: 3}\\
\hspace*{4mm}\texttt{backoff: exponential}\\
\hspace*{4mm}\texttt{base\_delay: 1s}\\
\texttt{connection\_pool:}\\
\hspace*{4mm}\texttt{max\_connections: 100}\\
\hspace*{4mm}\texttt{idle\_timeout: 300s}
}}
\end{figure}

\subsection{Serialization Layer}

Efficient model serialization is critical for cross-border FL where bandwidth is constrained. The framework implements:

\textbf{Protocol Buffer-style Binary Format:}
\begin{itemize}
    \item Tensor metadata (shape, dtype) + raw binary data
    \item 30\% smaller than JSON, 15\% smaller than pickle
    \item Cross-platform compatibility (Python, C++, Java)
\end{itemize}

\textbf{Delta Serialization:}
\begin{itemize}
    \item Transmits only changed parameters between rounds
    \item Sparse encoding for gradient updates
    \item Up to 90\% bandwidth reduction for fine-tuning scenarios
\end{itemize}

\textbf{EHDS-Compliant Serialization:}
\begin{itemize}
    \item Embeds permit ID, timestamp, and provenance metadata
    \item Cryptographic signatures for integrity verification
    \item Audit trail fields for GDPR Article 30 compliance
\end{itemize}

\subsection{Caching Layer}

Distributed caching accelerates FL operations and provides fault tolerance:

\textbf{Redis-based Implementation:}
\begin{itemize}
    \item \textbf{Model Checkpoints}: Global model snapshots for recovery
    \item \textbf{Client State}: Local model states, optimizer moments
    \item \textbf{Metrics}: Real-time training metrics for monitoring
\end{itemize}

\textbf{Features:}
\begin{itemize}
    \item LRU/LFU/TTL eviction policies
    \item Distributed locking for concurrent access
    \item Automatic serialization/deserialization
    \item Cache warming for predictable latency
\end{itemize}

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\columnwidth}{
\small
\textbf{Algorithm E.1: Distributed Lock for Aggregation}\\[2pt]
\textbf{Input:} Lock name, TTL, client ID\\
\textbf{Output:} Lock acquired (boolean)\\[4pt]
// Attempt to acquire lock atomically\\
acquired $\leftarrow$ Redis.SET(lock\_name, client\_id, NX, EX=TTL)\\[2pt]
\textbf{if} acquired \textbf{then}\\
\hspace*{4mm}// Perform aggregation\\
\hspace*{4mm}PerformAggregation()\\
\hspace*{4mm}// Release lock only if we own it\\
\hspace*{4mm}\textbf{if} Redis.GET(lock\_name) == client\_id \textbf{then}\\
\hspace*{8mm}Redis.DEL(lock\_name)\\[2pt]
\textbf{return} acquired
}}
\end{figure}

\subsection{Orchestration Layer}

Enterprise FL deployments require automated scaling and management:

\textbf{Kubernetes Integration:}
\begin{itemize}
    \item Deploys FL clients/aggregators as Kubernetes pods
    \item Horizontal Pod Autoscaler (HPA) for elastic scaling
    \item ConfigMaps for FL hyperparameter injection
    \item Secrets for credential management (HDAB API keys)
\end{itemize}

\textbf{Ray Distributed Computing:}
\begin{itemize}
    \item Actor-based FL client/aggregator implementation
    \item Automatic resource management and fault tolerance
    \item Integration with Ray Tune for federated HPO
    \item Object store for efficient gradient sharing
\end{itemize}

\textbf{Auto-Scaling Policies:}
\begin{itemize}
    \item \textbf{Reactive}: Scale based on queue depth/latency
    \item \textbf{Predictive}: ML-based workload forecasting
    \item \textbf{Scheduled}: Time-based scaling for known patterns
\end{itemize}

\subsection{Monitoring Layer}

Production observability follows the three pillars: metrics, logs, and traces.

\textbf{Prometheus Metrics:}
\begin{itemize}
    \item \textbf{Counters}: rounds\_total, permits\_validated, gradients\_received
    \item \textbf{Gauges}: active\_clients, global\_model\_loss, privacy\_budget\_remaining
    \item \textbf{Histograms}: round\_duration, communication\_latency, aggregation\_time
    \item \textbf{Summaries}: gradient\_norm\_quantiles
\end{itemize}

\textbf{Grafana Dashboards:}
The framework auto-generates Grafana dashboard JSON for:
\begin{itemize}
    \item FL training progress (loss, accuracy per round)
    \item Client participation and health
    \item Communication latency heatmaps
    \item Privacy budget consumption
    \item EHDS compliance status
\end{itemize}

\textbf{Alerting:}
\begin{itemize}
    \item Privacy budget exhaustion warning
    \item Client dropout rate exceeding threshold
    \item Model divergence detection
    \item Permit expiration alerts
\end{itemize}

\subsection{Model Watermarking}

Intellectual property protection for FL models trained on EHDS data:

\textbf{Watermarking Techniques:}
\begin{itemize}
    \item \textbf{Spread Spectrum}: Embeds watermark in frequency domain, robust to fine-tuning
    \item \textbf{LSB (Least Significant Bit)}: Embeds in low-order bits of weights
    \item \textbf{Backdoor-based}: Specific input-output pairs as ownership proof
    \item \textbf{Passport Layers}: Dedicated layers encode ownership information
\end{itemize}

\textbf{EHDS Use Case:}
Watermarking enables tracking of model provenance---which data permits, which hospitals contributed, and usage restrictions. This supports EHDS requirements for accountability in secondary use.

\subsection{Cross-Silo Enhancements}

Enterprise FL deployments require advanced capabilities beyond basic aggregation. The framework implements three cross-silo enhancements for production EHDS deployments.

\textbf{Multi-Model Federation:}
Ensemble learning across federated models improves robustness and generalization:
\begin{itemize}
    \item \textbf{Weighted Voting}: Performance-based weighting of ensemble members
    \item \textbf{Stacking}: Meta-learner trained on base model outputs
    \item \textbf{Mixture of Experts}: Input-dependent gating selects expert models
    \item \textbf{Diversity Enforcement}: Ensures ensemble members capture different patterns
\end{itemize}

\textbf{Automatic Algorithm Selection:}
Data-driven selection of the optimal FL algorithm:
\begin{itemize}
    \item \textbf{Task Analysis}: Automatic detection of data heterogeneity (IID vs.\ non-IID)
    \item \textbf{Multi-Armed Bandit}: UCB/Thompson Sampling for exploration-exploitation
    \item \textbf{Criterion-based}: Optimize for accuracy, convergence, fairness, or privacy
\end{itemize}

\textbf{Adaptive Aggregation:}
Dynamic switching between aggregation algorithms based on runtime metrics:

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.92\columnwidth}{
\small
\textbf{Algorithm E.2: Adaptive Aggregation}\\[2pt]
\textbf{Input:} Client updates, metrics history, cooldown period\\
\textbf{Output:} Aggregated model, selected algorithm\\[4pt]
// Evaluate current algorithm performance\\
score $\leftarrow$ WeightedScore(loss, accuracy, variance, convergence)\\[2pt]
// Check if switch would be beneficial\\
\textbf{if} RoundsSinceSwitch $>$ CooldownPeriod \textbf{then}\\
\hspace*{4mm}\textbf{for each} candidate $\in$ \{FedAvg, FedProx, SCAFFOLD, ...\} \textbf{do}\\
\hspace*{8mm}alt\_score $\leftarrow$ EstimatePerformance(candidate)\\
\hspace*{8mm}\textbf{if} alt\_score $>$ score + SwitchThreshold \textbf{then}\\
\hspace*{12mm}SwitchTo(candidate)\\[2pt]
// Perform aggregation with current algorithm\\
aggregated $\leftarrow$ CurrentAlgorithm.Aggregate(updates)\\[2pt]
\textbf{return} aggregated
}}
\end{figure}

\textbf{Supported Algorithms:}
\begin{itemize}
    \item \textbf{Standard}: FedAvg, FedProx, SCAFFOLD, FedNova
    \item \textbf{Adaptive}: FedAdam, FedYogi, FedAdagrad
    \item \textbf{Byzantine-resilient}: Krum, TrimmedMean, Median, Bulyan
\end{itemize}

\textbf{EHDS Relevance:}
Cross-border federations exhibit dynamic characteristics: client availability varies by time zone, data heterogeneity depends on participating Member States, and regulatory requirements may change. Adaptive aggregation automatically adjusts to these conditions without manual intervention.

% ============================================================================
% APPENDIX F - EXTENDED EHDS INTEROPERABILITY
% ============================================================================
\section{Extended EHDS Interoperability}
\label{appendix:interoperability}

Beyond the FHIR R4 preprocessing described in Algorithm~7, the FL-EHDS framework implements additional interoperability standards required for cross-border health data exchange.

\subsection{OMOP Common Data Model}

The Observational Medical Outcomes Partnership (OMOP) CDM provides a standardized analytical format for observational health data. Many European research networks (e.g., EHDEN, OHDSI) use OMOP.

\textbf{Framework Support:}
\begin{itemize}
    \item \textbf{ETL Pipelines}: Transform source EHR to OMOP CDM v5.4
    \item \textbf{Vocabulary Mapping}: Standard concepts (SNOMED, ICD10, LOINC, RxNorm)
    \item \textbf{Cohort Definitions}: ATLAS-compatible cohort SQL generation
    \item \textbf{Feature Extraction}: FeatureExtraction package integration for ML-ready datasets
\end{itemize}

\textbf{FL Integration:}
OMOP standardization enables consistent feature engineering across sites:
\begin{enumerate}
    \item Each hospital transforms local EHR to OMOP
    \item Feature extraction produces identical schema across sites
    \item FL training proceeds on homogeneous feature spaces
\end{enumerate}

\subsection{IHE Integration Profiles}

Integrating the Healthcare Enterprise (IHE) profiles ensure secure, auditable data exchange:

\textbf{Implemented Profiles:}
\begin{itemize}
    \item \textbf{ATNA (Audit Trail and Node Authentication)}:
    \begin{itemize}
        \item TLS mutual authentication between FL nodes
        \item Syslog audit messages for all data access events
        \item RFC 5424 compliant audit record format
    \end{itemize}

    \item \textbf{BPPC (Basic Patient Privacy Consents)}:
    \begin{itemize}
        \item Maps Article 71 opt-out to BPPC consent documents
        \item XDS.b integration for consent document sharing
        \item Consent enforcement at FL training initiation
    \end{itemize}

    \item \textbf{XCA (Cross-Community Access)}:
    \begin{itemize}
        \item Cross-border document query and retrieve
        \item Initiating/Responding Gateway implementation
        \item Patient identity correlation across communities
    \end{itemize}

    \item \textbf{PIX/PDQ (Patient Identifier Cross-referencing/Demographics Query)}:
    \begin{itemize}
        \item Patient matching across institutional boundaries
        \item Pseudonymization-aware identity management
        \item Integration with national eHealth infrastructures
    \end{itemize}

    \item \textbf{XUA (Cross-Enterprise User Assertion)}:
    \begin{itemize}
        \item SAML 2.0 assertions for federated authentication
        \item Role-based access control integration
        \item HDAB authorization token propagation
    \end{itemize}
\end{itemize}

\subsection{Cross-Border Data Exchange}

The framework implements the technical specifications for HealthData@EU cross-border exchange:

\textbf{Message Formats:}
\begin{itemize}
    \item EHDS Data Permit Exchange Format (JSON-LD)
    \item Federated Query Protocol (based on SPARQL Federation)
    \item Model Update Message Format (Protocol Buffers)
\end{itemize}

\textbf{Security Requirements:}
\begin{itemize}
    \item eIDAS-compliant electronic signatures for permits
    \item TLS 1.3 for all cross-border communication
    \item Certificate-based node authentication (EU trust framework)
\end{itemize}

\textbf{Metadata Standards:}
\begin{itemize}
    \item DCAT-AP Health extension for dataset cataloging
    \item Provenance metadata (W3C PROV-O)
    \item Data quality indicators per EMA guidelines
\end{itemize}

\subsection{Reference Implementation Architecture}

Figure~\ref{fig:interop_architecture} illustrates how the interoperability components integrate within the FL-EHDS framework.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=0.8cm,
    box/.style={draw, rounded corners, minimum width=2.5cm, minimum height=0.7cm, font=\scriptsize},
    layer/.style={draw, dashed, rounded corners, minimum width=8cm, minimum height=1.2cm}
]
% Data Sources
\node[box, fill=blue!20] (ehr) {EHR Systems};
\node[box, fill=blue!20, right=0.3cm of ehr] (lab) {Lab Systems};
\node[box, fill=blue!20, right=0.3cm of lab] (pharm) {Pharmacy};

% Interoperability Layer
\node[box, fill=green!20, below=0.8cm of lab] (fhir) {FHIR R4};
\node[box, fill=green!20, left=0.3cm of fhir] (omop) {OMOP CDM};
\node[box, fill=green!20, right=0.3cm of fhir] (ihe) {IHE Profiles};

% FL Layer
\node[box, fill=orange!20, below=0.8cm of fhir] (fl) {FL Training Engine};

% Labels
\node[left=0.1cm of ehr, font=\scriptsize] {Sources};
\node[left=0.1cm of omop, font=\scriptsize] {Interop};
\node[left=0.1cm of fl, font=\scriptsize] {FL};

% Arrows
\draw[->, thick] (ehr) -- (omop);
\draw[->, thick] (lab) -- (fhir);
\draw[->, thick] (pharm) -- (ihe);
\draw[->, thick] (omop) -- (fl);
\draw[->, thick] (fhir) -- (fl);
\draw[->, thick] (ihe) -- (fl);
\end{tikzpicture}
\caption{Interoperability layer integrating heterogeneous data sources for FL training.}
\label{fig:interop_architecture}
\end{figure}

% ============================================================================
% APPENDIX B: CLINICAL IMAGING EXPERIMENTS
% ============================================================================
\section{Clinical Imaging Experiments}
\label{appendix:imaging}

To validate FL-EHDS beyond synthetic tabular data, we conducted experiments on real clinical imaging datasets. This appendix reports results on the Chest X-ray pneumonia detection dataset, demonstrating the framework's applicability to medical imaging tasks.

\subsection{Dataset Description}

The Chest X-ray dataset~\cite{chestxray2018} comprises 5,856 pediatric chest radiographs from Guangzhou Women and Children's Medical Center, labeled for pneumonia detection (binary classification):
\begin{itemize}
    \item \textbf{Classes}: NORMAL (1,583), PNEUMONIA (4,273)
    \item \textbf{Image size}: Variable, resized to 224$\times$224 for training
    \item \textbf{Class imbalance}: 2.7:1 ratio (pneumonia-heavy)
    \item \textbf{EHDS relevance}: Pulmonology/Radiology, cross-border screening potential
\end{itemize}

\subsection{Model Architecture}

We employ HealthcareCNN, a 5-block convolutional network designed for medical imaging:
\begin{itemize}
    \item \textbf{Feature extraction}: 5 convolutional blocks with BatchNorm, ReLU, MaxPool, and Dropout (0.15--0.3)
    \item \textbf{Channel progression}: 32 $\rightarrow$ 64 $\rightarrow$ 128 $\rightarrow$ 256 $\rightarrow$ 512
    \item \textbf{Classifier}: Flatten $\rightarrow$ FC(512) $\rightarrow$ FC(128) $\rightarrow$ FC(2)
    \item \textbf{Parameters}: $\sim$12M trainable parameters
\end{itemize}

\subsection{Experimental Configuration}

\begin{itemize}
    \item \textbf{FL setup}: 5 simulated hospitals, data partitioned via Dirichlet distribution ($\alpha$=0.5 for Non-IID)
    \item \textbf{Training}: 20 rounds, 3 local epochs, batch size 32, Adam optimizer (lr=0.001)
    \item \textbf{Privacy}: Differential privacy with gradient clipping $C$=1.0
    \item \textbf{Hardware}: Apple M-series CPU (MPS not used for reproducibility)
    \item \textbf{Reproducibility}: 3 random seeds (42, 123, 456)
\end{itemize}

\subsection{Results}

Table~\ref{tab:imaging_results} reports FL performance on the Chest X-ray pneumonia detection task.

\begin{table}[htbp]
\centering
\caption{FL Results on Chest X-ray Pneumonia Detection}
\label{tab:imaging_results}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Configuration} & \textbf{Acc.} & \textbf{F1} & \textbf{AUC} & \textbf{Prec.} & \textbf{Rec.} \\
\midrule
FedAvg (IID) & 87.3\%$\pm$1.2 & 0.89$\pm$0.01 & 0.85$\pm$0.02 & 0.86 & 0.92 \\
FedAvg (Non-IID) & 84.8\%$\pm$2.1 & 0.87$\pm$0.02 & 0.82$\pm$0.03 & 0.83 & 0.91 \\
FedProx (Non-IID) & 85.4\%$\pm$1.8 & 0.88$\pm$0.01 & 0.83$\pm$0.02 & 0.84 & 0.91 \\
FedAvg + DP ($\varepsilon$=10) & 81.2\%$\pm$2.4 & 0.84$\pm$0.03 & 0.78$\pm$0.04 & 0.80 & 0.89 \\
FedAvg + DP ($\varepsilon$=1) & 76.5\%$\pm$3.1 & 0.80$\pm$0.04 & 0.73$\pm$0.05 & 0.75 & 0.86 \\
\bottomrule
\end{tabular}

\vspace{1mm}
\footnotesize{5 hospitals, 20 rounds, 3 local epochs, batch size 32. HealthcareCNN model. Results: mean $\pm$ std over 3 seeds.}
\end{table}

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{High baseline performance}: FedAvg achieves 87.3\% accuracy on IID data, demonstrating that FL can match centralized performance for medical imaging tasks.

    \item \textbf{Non-IID robustness}: The accuracy drop from IID to Non-IID (2.5pp) is smaller than for tabular data, suggesting CNNs are more robust to label skew.

    \item \textbf{FedProx benefit}: Proximal regularization improves Non-IID performance by 0.6pp, validating its utility for heterogeneous hospital populations.

    \item \textbf{Privacy cost}: DP with $\varepsilon$=10 incurs 3.6pp accuracy drop; $\varepsilon$=1 costs 8.3pp---higher than tabular but still clinically viable for screening applications.

    \item \textbf{Class imbalance}: The pneumonia-heavy dataset biases recall (91--92\%) over precision (83--86\%), appropriate for screening where false negatives are costly.
\end{enumerate}

\subsection{EHDS Compliance Demonstration}

This experiment demonstrates a complete EHDS-compliant workflow:
\begin{enumerate}
    \item \textbf{Data permit}: Simulated Article 53(1)(b) authorization for scientific research
    \item \textbf{Opt-out}: 2\% of records randomly excluded (simulating Article 71 opt-outs)
    \item \textbf{Privacy}: $(\varepsilon=10, \delta=10^{-5})$-DP with RDP accounting
    \item \textbf{Audit}: Complete training log with per-round metrics and client participation
    \item \textbf{Result}: Federated pneumonia classifier achieving 85\%+ accuracy without data centralization
\end{enumerate}

\subsection{Reproducibility}

All imaging experiments are reproducible via:
\begin{verbatim}
cd fl-ehds-framework/benchmarks
python run_imaging_experiments.py \
    --dataset chest_xray --seeds 42 123 456
\end{verbatim}

Results, LaTeX tables, and convergence plots are auto-saved to \texttt{results/imaging\_benchmarks/}.

% ============================================================================

\subsection{Reproducibility}

All interoperability components are available in the reference implementation:
\begin{verbatim}
fl-ehds-framework/core/
 fhir_integration.py    # FHIR R4
 omop_cdm.py            # OMOP CDM
 ihe_profiles.py        # IHE Profiles
 hdab_api.py            # HDAB API
\end{verbatim}

\noindent Example usage for OMOP feature extraction:
\begin{verbatim}
from fl_ehds.core import OMOPFeatureExtractor

extractor = OMOPFeatureExtractor(
    cdm_schema="omop_cdm",
    cohort_definition_id=123
)
features = extractor.extract_features(
    feature_specs=["demographics", "conditions",
                   "medications", "measurements"]
)
\end{verbatim}

\end{document}
