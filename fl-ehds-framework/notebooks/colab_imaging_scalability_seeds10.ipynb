{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL-EHDS: Scalability Gap + Imaging 10-Seed Statistical Validation\n",
    "\n",
    "Two experiment batches in one notebook:\n",
    "\n",
    "**Part A \u2014 Scalability gap (9 experiments)**\n",
    "Completes the 9 missing experiments in `checkpoint_imaging_scalability.json`\n",
    "(Skin_Cancer K10/K20 s42 + chest_xray K20 s42, all 3 algos).\n",
    "\n",
    "**Part B \u2014 10-Seed Validation (90 experiments)**\n",
    "FedAvg/Ditto/HPFL \u00d7 3 datasets \u00d7 10 seeds for Wilcoxon statistical test\n",
    "on imaging (same as tabular Phase 4).\n",
    "\n",
    "**Setup:** Runtime > Change runtime type > **T4 GPU**\n",
    "\n",
    "**Total:** 9 + 90 = **99 experiments**\n",
    "\n",
    "**Checkpoint:** Saved to Google Drive after **every round**, per-client, per-epoch.\n",
    "If the session disconnects, re-run from Section 3 \u2014 it auto-resumes.\n",
    "\n",
    "**Estimated time:** ~5 hours on T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent checkpoint storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "DRIVE_OUTPUT = '/content/drive/MyDrive/FL-EHDS-FLICS2026/colab_results'\n",
    "os.makedirs(DRIVE_OUTPUT, exist_ok=True)\n",
    "print(f'Drive output: {DRIVE_OUTPUT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    mem = getattr(props, 'total_memory', None) or getattr(props, 'total_mem', 0)\n",
    "    print(f'Memory: {mem / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/FabioLiberti/FL-EHDS-FLICS2026.git /content/FL-EHDS-FLICS2026 2>/dev/null || (cd /content/FL-EHDS-FLICS2026 && git pull)\n",
    "%cd /content/FL-EHDS-FLICS2026/fl-ehds-framework\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q scikit-learn scipy tqdm Pillow structlog cryptography grpcio aiohttp pydantic pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kagglehub\n",
    "\n",
    "import os\n",
    "os.environ['KAGGLE_API_TOKEN'] = 'KGAT_edd561c1bc682c9ad06930bacd164431'\n",
    "\n",
    "import kagglehub\n",
    "print(f'kagglehub version: {kagglehub.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import shutil, glob\n",
    "\n",
    "cache_path = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
    "os.makedirs('data/chest_xray', exist_ok=True)\n",
    "for item in ['train', 'test', 'val']:\n",
    "    src = os.path.join(cache_path, 'chest_xray', item)\n",
    "    if not os.path.exists(src):\n",
    "        src = os.path.join(cache_path, item)\n",
    "    dst = f'data/chest_xray/{item}'\n",
    "    if os.path.exists(src) and not os.path.exists(dst):\n",
    "        shutil.copytree(src, dst)\n",
    "shutil.rmtree('data/chest_xray/__MACOSX', ignore_errors=True)\n",
    "print('Chest X-Ray ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cache_path = kagglehub.dataset_download(\"fanconic/skin-cancer-malignant-vs-benign\")\n",
    "dst = 'data/Skin Cancer'\n",
    "if not os.path.exists(dst):\n",
    "    shutil.copytree(cache_path, dst)\n",
    "print('Skin Cancer ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cache_path = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")\n",
    "os.makedirs('data/Brain_Tumor', exist_ok=True)\n",
    "for root, dirs, files in os.walk(cache_path):\n",
    "    for d in dirs:\n",
    "        d_lower = d.lower()\n",
    "        if d_lower in ['glioma', 'meningioma', 'pituitary', 'notumor', 'no_tumor', 'healthy']:\n",
    "            target = 'healthy' if d_lower in ['notumor', 'no_tumor'] else d_lower\n",
    "            src = os.path.join(root, d)\n",
    "            dst_dir = f'data/Brain_Tumor/{target}'\n",
    "            if not os.path.exists(dst_dir):\n",
    "                shutil.copytree(src, dst_dir)\n",
    "            else:\n",
    "                for f in os.listdir(src):\n",
    "                    sf, df = os.path.join(src, f), os.path.join(dst_dir, f)\n",
    "                    if os.path.isfile(sf) and not os.path.exists(df):\n",
    "                        shutil.copy2(sf, df)\n",
    "print('Brain Tumor ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Dataset Summary ===')\n",
    "for ds_name, ds_path in [('Chest X-Ray', 'data/chest_xray'),\n",
    "                          ('Skin Cancer', 'data/Skin Cancer'),\n",
    "                          ('Brain Tumor', 'data/Brain_Tumor')]:\n",
    "    count = sum(1 for _ in glob.iglob(f'{ds_path}/**/*.*', recursive=True)\n",
    "                if _.lower().endswith(('.jpg', '.jpeg', '.png')))\n",
    "    subdirs = [d for d in os.listdir(ds_path) if os.path.isdir(os.path.join(ds_path, d))]\n",
    "    print(f'  {ds_name:15s}: {count:5d} images, classes: {subdirs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Shared Utilities & Training Function\n",
    "\n",
    "Run this cell once \u2014 used by both Part A and Part B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import tempfile\n",
    "import traceback\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, '/content/FL-EHDS-FLICS2026/fl-ehds-framework')\n",
    "\n",
    "from terminal.fl_trainer import ImageFederatedTrainer, _detect_device\n",
    "\n",
    "# ======================================================================\n",
    "# Shared configuration\n",
    "# ======================================================================\n",
    "\n",
    "IMAGING_DATASETS = {\n",
    "    \"chest_xray\": {\"data_dir\": \"data/chest_xray\", \"num_classes\": 2, \"short\": \"CX\"},\n",
    "    \"Brain_Tumor\": {\"data_dir\": \"data/Brain_Tumor\", \"num_classes\": 4, \"short\": \"BT\"},\n",
    "    \"Skin_Cancer\": {\"data_dir\": \"data/Skin Cancer\", \"num_classes\": 2, \"short\": \"SC\"},\n",
    "}\n",
    "\n",
    "IMAGING_CONFIG = dict(\n",
    "    num_clients=5, num_rounds=20, local_epochs=2, batch_size=32,\n",
    "    learning_rate=0.001, model_type=\"resnet18\", is_iid=False, alpha=0.5,\n",
    "    freeze_backbone=False, freeze_level=2, use_fedbn=True,\n",
    "    use_class_weights=True, use_amp=True, mu=0.1,\n",
    ")\n",
    "\n",
    "DATASET_OVERRIDES = {\"Brain_Tumor\": {\"learning_rate\": 0.0005}}\n",
    "\n",
    "EARLY_STOPPING = dict(enabled=True, patience=4, min_delta=0.003, min_rounds=8, metric=\"accuracy\")\n",
    "\n",
    "OUTPUT_DIR = Path(DRIVE_OUTPUT)\n",
    "\n",
    "# ======================================================================\n",
    "# Utilities\n",
    "# ======================================================================\n",
    "\n",
    "_log_file = None\n",
    "\n",
    "def log(msg, also_print=True):\n",
    "    ts = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    line = f\"[{ts}] {msg}\"\n",
    "    if also_print:\n",
    "        print(line, flush=True)\n",
    "    if _log_file:\n",
    "        try:\n",
    "            _log_file.write(line + \"\\n\")\n",
    "            _log_file.flush()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def save_checkpoint(data, checkpoint_file):\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    path = OUTPUT_DIR / checkpoint_file\n",
    "    bak = OUTPUT_DIR / (checkpoint_file + \".bak\")\n",
    "    data[\"metadata\"][\"last_save\"] = datetime.now().isoformat()\n",
    "    fd, tmp = tempfile.mkstemp(dir=str(OUTPUT_DIR), prefix=\".ckpt_\", suffix=\".tmp\")\n",
    "    try:\n",
    "        with os.fdopen(fd, \"w\") as f:\n",
    "            json.dump(data, f, indent=2, default=str)\n",
    "            f.flush()\n",
    "            os.fsync(f.fileno())\n",
    "        if path.exists():\n",
    "            shutil.copy2(str(path), str(bak))\n",
    "        os.replace(tmp, str(path))\n",
    "    except Exception:\n",
    "        try:\n",
    "            os.unlink(tmp)\n",
    "        except OSError:\n",
    "            pass\n",
    "        raise\n",
    "\n",
    "def load_checkpoint(checkpoint_file):\n",
    "    for p in [OUTPUT_DIR / checkpoint_file, OUTPUT_DIR / (checkpoint_file + \".bak\")]:\n",
    "        if p.exists():\n",
    "            try:\n",
    "                with open(p) as f:\n",
    "                    return json.load(f)\n",
    "            except (json.JSONDecodeError, IOError):\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def _cleanup_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "class EarlyStoppingMonitor:\n",
    "    def __init__(self, patience=4, min_delta=0.003, min_rounds=8, metric=\"accuracy\"):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.min_rounds = min_rounds\n",
    "        self.metric = metric\n",
    "        self.best_value = -float('inf')\n",
    "        self.best_round = 0\n",
    "        self.counter = 0\n",
    "\n",
    "    def check(self, round_num, metrics):\n",
    "        value = metrics.get(self.metric, 0)\n",
    "        if value > self.best_value + self.min_delta:\n",
    "            self.best_value = value\n",
    "            self.best_round = round_num\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        if round_num < self.min_rounds:\n",
    "            return False\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "def _evaluate_per_client(trainer):\n",
    "    model = trainer.global_model\n",
    "    model.eval()\n",
    "    per_client = {}\n",
    "    with torch.no_grad():\n",
    "        for cid in range(trainer.num_clients):\n",
    "            X, y = trainer.client_test_data[cid]\n",
    "            X_t = torch.FloatTensor(X).to(trainer.device) if isinstance(X, np.ndarray) else X.to(trainer.device)\n",
    "            y_t = torch.LongTensor(y).to(trainer.device) if isinstance(y, np.ndarray) else y.to(trainer.device)\n",
    "            correct = total = 0\n",
    "            for i in range(0, len(y_t), 64):\n",
    "                out = model(X_t[i:i+64])\n",
    "                preds = out.argmax(dim=1)\n",
    "                correct += (preds == y_t[i:i+64]).sum().item()\n",
    "                total += len(y_t[i:i+64])\n",
    "            per_client[str(cid)] = correct / total if total > 0 else 0.0\n",
    "    return per_client\n",
    "\n",
    "def _compute_fairness(per_client_acc):\n",
    "    accs = list(per_client_acc.values())\n",
    "    if not accs:\n",
    "        return {}\n",
    "    jain = (sum(accs)**2) / (len(accs) * sum(a**2 for a in accs)) if accs else 0\n",
    "    sorted_a = sorted(accs)\n",
    "    n = len(sorted_a)\n",
    "    cumsum = np.cumsum(sorted_a)\n",
    "    gini = (2 * sum((i+1)*v for i, v in enumerate(sorted_a))) / (n * cumsum[-1]) - (n+1)/n if cumsum[-1] > 0 else 0\n",
    "    return {\n",
    "        \"mean\": round(float(np.mean(accs)), 4),\n",
    "        \"std\": round(float(np.std(accs)), 4),\n",
    "        \"min\": round(float(min(accs)), 4),\n",
    "        \"max\": round(float(max(accs)), 4),\n",
    "        \"jain_index\": round(float(jain), 4),\n",
    "        \"gini\": round(float(max(0, gini)), 4),\n",
    "    }\n",
    "\n",
    "# ======================================================================\n",
    "# Training function (generic \u2014 works for both Part A and Part B)\n",
    "# ======================================================================\n",
    "\n",
    "def run_single_experiment(\n",
    "    ds_name, data_dir, algorithm, seed, num_clients,\n",
    "    config, es_config,\n",
    "    exp_idx, total_exps,\n",
    "    checkpoint_data, checkpoint_file, exp_key,\n",
    "    trainer_ckpt_path,\n",
    "):\n",
    "    start = time.time()\n",
    "    num_rounds = config[\"num_rounds\"]\n",
    "\n",
    "    cfg = {**config, \"num_clients\": num_clients}\n",
    "    if ds_name in DATASET_OVERRIDES:\n",
    "        cfg.update(DATASET_OVERRIDES[ds_name])\n",
    "\n",
    "    trainer = ImageFederatedTrainer(\n",
    "        data_dir=data_dir,\n",
    "        num_clients=num_clients,\n",
    "        algorithm=algorithm,\n",
    "        local_epochs=cfg[\"local_epochs\"],\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        learning_rate=cfg[\"learning_rate\"],\n",
    "        is_iid=cfg[\"is_iid\"],\n",
    "        alpha=cfg[\"alpha\"],\n",
    "        mu=cfg.get(\"mu\", 0.1),\n",
    "        seed=seed,\n",
    "        model_type=cfg[\"model_type\"],\n",
    "        freeze_backbone=cfg.get(\"freeze_backbone\", False),\n",
    "        freeze_level=cfg.get(\"freeze_level\"),\n",
    "        use_fedbn=cfg.get(\"use_fedbn\", False),\n",
    "        use_class_weights=cfg.get(\"use_class_weights\", True),\n",
    "        use_amp=cfg.get(\"use_amp\", True),\n",
    "    )\n",
    "    trainer.num_rounds = num_rounds\n",
    "\n",
    "    es = EarlyStoppingMonitor(\n",
    "        **{k: v for k, v in es_config.items() if k != \"enabled\"}\n",
    "    ) if es_config.get(\"enabled\") else None\n",
    "\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "    best_round = 0\n",
    "    start_round = 0\n",
    "\n",
    "    # Resume mid-experiment\n",
    "    in_prog = checkpoint_data.get(\"in_progress\") if checkpoint_data else None\n",
    "    if (in_prog and in_prog.get(\"key\") == exp_key\n",
    "            and trainer_ckpt_path and Path(trainer_ckpt_path).exists()):\n",
    "        try:\n",
    "            start_round = trainer.load_checkpoint(trainer_ckpt_path)\n",
    "            history = in_prog.get(\"history\", [])\n",
    "            best_acc = in_prog.get(\"best_acc\", 0.0)\n",
    "            best_round = in_prog.get(\"best_round\", 0)\n",
    "            if es and history:\n",
    "                for h in history:\n",
    "                    es.check(h[\"round\"], {\"accuracy\": h[\"accuracy\"]})\n",
    "            log(f\"  RESUMED from round {start_round} (best={best_acc:.1%})\")\n",
    "        except Exception as e:\n",
    "            log(f\"  WARNING: resume failed ({e}), restarting from R1\")\n",
    "            start_round = 0\n",
    "            history = []\n",
    "            best_acc = 0.0\n",
    "            best_round = 0\n",
    "\n",
    "    for r in range(start_round, num_rounds):\n",
    "        rr = trainer.train_round(r)\n",
    "\n",
    "        client_metrics = [\n",
    "            {\n",
    "                \"client_id\": cr.client_id,\n",
    "                \"train_loss\": round(cr.train_loss, 6),\n",
    "                \"train_acc\": round(cr.train_acc, 6),\n",
    "                \"num_samples\": cr.num_samples,\n",
    "                \"epochs_completed\": cr.epochs_completed,\n",
    "                \"epoch_metrics\": cr.epoch_metrics or [],\n",
    "            }\n",
    "            for cr in rr.client_results\n",
    "        ]\n",
    "\n",
    "        metrics = {\n",
    "            \"round\": r + 1,\n",
    "            \"accuracy\": rr.global_acc,\n",
    "            \"loss\": rr.global_loss,\n",
    "            \"f1\": rr.global_f1,\n",
    "            \"precision\": rr.global_precision,\n",
    "            \"recall\": rr.global_recall,\n",
    "            \"auc\": rr.global_auc,\n",
    "            \"time_seconds\": round(rr.time_seconds, 2),\n",
    "            \"client_results\": client_metrics,\n",
    "        }\n",
    "        history.append(metrics)\n",
    "\n",
    "        if rr.global_acc > best_acc:\n",
    "            best_acc = rr.global_acc\n",
    "            best_round = r + 1\n",
    "\n",
    "        log(f\"[{exp_idx}/{total_exps}] {ds_name} | {algorithm} | K={num_clients} | s{seed} | \"\n",
    "            f\"R{r+1}/{num_rounds} | Acc:{rr.global_acc:.1%} | Best:{best_acc:.1%}(r{best_round})\")\n",
    "\n",
    "        # Save checkpoint after EVERY round\n",
    "        if trainer_ckpt_path:\n",
    "            try:\n",
    "                trainer.save_checkpoint(trainer_ckpt_path)\n",
    "            except Exception:\n",
    "                pass\n",
    "        checkpoint_data[\"in_progress\"] = {\n",
    "            \"key\": exp_key,\n",
    "            \"dataset\": ds_name,\n",
    "            \"algorithm\": algorithm,\n",
    "            \"seed\": seed,\n",
    "            \"num_clients\": num_clients,\n",
    "            \"round\": r + 1,\n",
    "            \"total_rounds\": num_rounds,\n",
    "            \"best_acc\": best_acc,\n",
    "            \"best_round\": best_round,\n",
    "            \"history\": history,\n",
    "            \"elapsed_seconds\": round(time.time() - start, 1),\n",
    "        }\n",
    "        save_checkpoint(checkpoint_data, checkpoint_file)\n",
    "\n",
    "        if es and es.check(r + 1, {\"accuracy\": rr.global_acc}):\n",
    "            log(f\"  -> Early stop at R{r+1} (best={best_acc:.1%} at r{best_round})\")\n",
    "            break\n",
    "\n",
    "    per_client_acc = _evaluate_per_client(trainer)\n",
    "    fairness = _compute_fairness(per_client_acc)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    result = {\n",
    "        \"dataset\": ds_name,\n",
    "        \"algorithm\": algorithm,\n",
    "        \"seed\": seed,\n",
    "        \"num_clients\": num_clients,\n",
    "        \"history\": history,\n",
    "        \"final_metrics\": history[-1] if history else {},\n",
    "        \"per_client_acc\": per_client_acc,\n",
    "        \"fairness\": fairness,\n",
    "        \"runtime_seconds\": round(elapsed, 1),\n",
    "        \"config\": cfg,\n",
    "        \"stopped_early\": es is not None and es.counter >= es.patience,\n",
    "        \"actual_rounds\": len(history),\n",
    "        \"best_metrics\": {\"accuracy\": best_acc, \"round\": best_round},\n",
    "        \"best_round\": best_round,\n",
    "    }\n",
    "\n",
    "    checkpoint_data.pop(\"in_progress\", None)\n",
    "    if trainer_ckpt_path:\n",
    "        try:\n",
    "            Path(trainer_ckpt_path).unlink(missing_ok=True)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "    del trainer\n",
    "    _cleanup_gpu()\n",
    "    return result\n",
    "\n",
    "print(f'Device: {_detect_device(None)}')\n",
    "print('Shared utilities & training function loaded OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Part A \u2014 Complete Scalability Gap (9 experiments)\n",
    "\n",
    "**9 missing experiments** from `checkpoint_imaging_scalability.json`:\n",
    "- Skin_Cancer \u00d7 FedAvg/Ditto/HPFL \u00d7 K10 \u00d7 seed=42 (3)\n",
    "- Skin_Cancer \u00d7 FedAvg/Ditto/HPFL \u00d7 K20 \u00d7 seed=42 (3)\n",
    "- chest_xray \u00d7 FedAvg/Ditto/HPFL \u00d7 K20 \u00d7 seed=42 (3)\n",
    "\n",
    "Loads existing checkpoint from Drive (63 completed), adds the 9 missing, saves back.\n",
    "If already complete, this cell finishes instantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# PART A: Scalability gap \u2014 9 missing experiments\n",
    "# ======================================================================\n",
    "\n",
    "SCALE_CHECKPOINT = \"checkpoint_imaging_scalability.json\"\n",
    "SCALE_LOG = \"experiment_imaging_scalability.log\"\n",
    "SCALE_TRAINER_STATE = \".trainer_state_scalability.pt\"\n",
    "\n",
    "SCALE_ALGORITHMS = [\"FedAvg\", \"Ditto\", \"HPFL\"]\n",
    "SCALE_K_VALUES = [10, 20]\n",
    "SCALE_SEEDS = [42, 123, 456, 789]\n",
    "\n",
    "_log_file = open(OUTPUT_DIR / SCALE_LOG, \"a\")\n",
    "\n",
    "# Build the FULL scalability experiment list (same grid as original script)\n",
    "scale_experiments = []\n",
    "for k_val in SCALE_K_VALUES:\n",
    "    for ds_name in IMAGING_DATASETS:\n",
    "        for algo in SCALE_ALGORITHMS:\n",
    "            for seed in SCALE_SEEDS:\n",
    "                scale_experiments.append((ds_name, algo, seed, k_val))\n",
    "\n",
    "total_scale = len(scale_experiments)\n",
    "\n",
    "# Load existing checkpoint (should have 63 completed)\n",
    "scale_data = load_checkpoint(SCALE_CHECKPOINT)\n",
    "if scale_data:\n",
    "    done = len(scale_data.get(\"completed\", {}))\n",
    "    log(f\"PART A \u2014 Scalability: loaded {done}/{total_scale} completed\")\n",
    "else:\n",
    "    scale_data = {\n",
    "        \"completed\": {},\n",
    "        \"metadata\": {\n",
    "            \"total_experiments\": total_scale,\n",
    "            \"purpose\": \"Imaging scalability: K=10,20 for FedAvg/Ditto/HPFL (gap completion)\",\n",
    "            \"algorithms\": SCALE_ALGORITHMS,\n",
    "            \"k_values\": SCALE_K_VALUES,\n",
    "            \"datasets\": list(IMAGING_DATASETS.keys()),\n",
    "            \"seeds\": SCALE_SEEDS,\n",
    "            \"start_time\": datetime.now().isoformat(),\n",
    "            \"last_save\": None,\n",
    "            \"version\": \"imaging_scalability_v2_gap\",\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Count how many are actually missing\n",
    "missing = []\n",
    "for ds_name, algo, seed, k_val in scale_experiments:\n",
    "    key = f\"{ds_name}_{algo}_K{k_val}_s{seed}\"\n",
    "    if key not in scale_data.get(\"completed\", {}):\n",
    "        missing.append((ds_name, algo, seed, k_val, key))\n",
    "\n",
    "log(f\"PART A \u2014 {len(missing)} experiments to run (out of {total_scale} total)\")\n",
    "\n",
    "if missing:\n",
    "    log(f\"\\n{'='*66}\")\n",
    "    log(f\"  Part A: Scalability Gap Completion\")\n",
    "    log(f\"  {len(missing)} missing experiments\")\n",
    "    log(f\"{'='*66}\")\n",
    "\n",
    "    global_start_a = time.time()\n",
    "    completed_a = 0\n",
    "    trainer_ckpt_a = str(OUTPUT_DIR / SCALE_TRAINER_STATE)\n",
    "\n",
    "    for idx, (ds_name, algo, seed, k_val, key) in enumerate(missing, 1):\n",
    "        ds_info = IMAGING_DATASETS[ds_name]\n",
    "\n",
    "        elapsed = time.time() - global_start_a\n",
    "        eta = str(timedelta(seconds=int((len(missing) - completed_a) * elapsed / completed_a))) if completed_a > 0 else \"calculating...\"\n",
    "        log(f\"\\n--- [A:{idx}/{len(missing)}] {ds_name} | {algo} | K={k_val} | s{seed} | ETA: {eta} ---\")\n",
    "\n",
    "        try:\n",
    "            result = run_single_experiment(\n",
    "                ds_name=ds_name, data_dir=ds_info[\"data_dir\"],\n",
    "                algorithm=algo, seed=seed, num_clients=k_val,\n",
    "                config=IMAGING_CONFIG, es_config=EARLY_STOPPING,\n",
    "                exp_idx=idx, total_exps=len(missing),\n",
    "                checkpoint_data=scale_data, checkpoint_file=SCALE_CHECKPOINT,\n",
    "                exp_key=key, trainer_ckpt_path=trainer_ckpt_a,\n",
    "            )\n",
    "\n",
    "            scale_data[\"completed\"][key] = result\n",
    "            completed_a += 1\n",
    "            save_checkpoint(scale_data, SCALE_CHECKPOINT)\n",
    "\n",
    "            best_acc = result.get(\"best_metrics\", {}).get(\"accuracy\", 0)\n",
    "            es_info = f\" ES@R{result['actual_rounds']}\" if result.get(\"stopped_early\") else \"\"\n",
    "            log(f\"--- Done: Best={best_acc:.1%}{es_info} | {result['runtime_seconds']:.0f}s | [{completed_a}/{len(missing)}] ---\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"ERROR in {key}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            scale_data[\"completed\"][key] = {\n",
    "                \"dataset\": ds_name, \"algorithm\": algo, \"seed\": seed,\n",
    "                \"num_clients\": k_val, \"error\": str(e),\n",
    "            }\n",
    "            save_checkpoint(scale_data, SCALE_CHECKPOINT)\n",
    "\n",
    "    elapsed_a = time.time() - global_start_a\n",
    "    log(f\"\\nPart A COMPLETED: {completed_a}/{len(missing)} in {timedelta(seconds=int(elapsed_a))}\")\n",
    "else:\n",
    "    log(\"Part A: All scalability experiments already complete!\")\n",
    "\n",
    "total_scale_done = len(scale_data.get(\"completed\", {}))\n",
    "log(f\"Scalability checkpoint: {total_scale_done}/{total_scale} total\")\n",
    "\n",
    "if _log_file:\n",
    "    _log_file.close()\n",
    "    _log_file = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Part B \u2014 10-Seed Statistical Validation (90 experiments)\n",
    "\n",
    "**90 experiments** = 3 algos (FedAvg, Ditto, HPFL) \u00d7 3 datasets \u00d7 10 seeds\n",
    "\n",
    "Enables Wilcoxon signed-rank test on imaging results (same as tabular Phase 4).\n",
    "10 seeds provide sufficient statistical power for non-parametric testing.\n",
    "\n",
    "Seeds: 42, 123, 456, 789, 999, 7, 13, 31, 67, 101\n",
    "\n",
    "Checkpoint saved to Drive after **every training round**, with per-client and per-epoch metrics.\n",
    "If Colab disconnects, re-run this cell \u2014 it auto-resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# PART B: 10-Seed Statistical Validation \u2014 90 experiments\n",
    "# ======================================================================\n",
    "\n",
    "SEEDS10_CHECKPOINT = \"checkpoint_imaging_seeds10.json\"\n",
    "SEEDS10_LOG = \"experiment_imaging_seeds10.log\"\n",
    "SEEDS10_TRAINER_STATE = \".trainer_state_seeds10.pt\"\n",
    "\n",
    "SEEDS10_ALGORITHMS = [\"FedAvg\", \"Ditto\", \"HPFL\"]\n",
    "SEEDS10 = [42, 123, 456, 789, 999, 7, 13, 31, 67, 101]\n",
    "\n",
    "_log_file = open(OUTPUT_DIR / SEEDS10_LOG, \"a\")\n",
    "\n",
    "seeds10_experiments = []\n",
    "for ds_name in IMAGING_DATASETS:\n",
    "    for algo in SEEDS10_ALGORITHMS:\n",
    "        for seed in SEEDS10:\n",
    "            seeds10_experiments.append((ds_name, algo, seed))\n",
    "\n",
    "total_seeds10 = len(seeds10_experiments)\n",
    "\n",
    "seeds10_data = load_checkpoint(SEEDS10_CHECKPOINT)\n",
    "if seeds10_data:\n",
    "    done = len(seeds10_data.get(\"completed\", {}))\n",
    "    log(f\"PART B \u2014 Seeds10: AUTO-RESUMED {done}/{total_seeds10} completed\")\n",
    "else:\n",
    "    seeds10_data = {\n",
    "        \"completed\": {},\n",
    "        \"metadata\": {\n",
    "            \"total_experiments\": total_seeds10,\n",
    "            \"purpose\": \"Imaging 10-seed statistical validation for Wilcoxon signed-rank test\",\n",
    "            \"algorithms\": SEEDS10_ALGORITHMS,\n",
    "            \"datasets\": list(IMAGING_DATASETS.keys()),\n",
    "            \"seeds\": SEEDS10,\n",
    "            \"num_clients\": 5,\n",
    "            \"start_time\": datetime.now().isoformat(),\n",
    "            \"last_save\": None,\n",
    "            \"version\": \"imaging_seeds10_v1\",\n",
    "        }\n",
    "    }\n",
    "\n",
    "log(f\"\\n{'='*66}\")\n",
    "log(f\"  Part B: Imaging 10-Seed Statistical Validation\")\n",
    "log(f\"  {total_seeds10} experiments = {len(SEEDS10_ALGORITHMS)} algos x {len(IMAGING_DATASETS)} DS x {len(SEEDS10)} seeds\")\n",
    "log(f\"  Algorithms: {SEEDS10_ALGORITHMS}\")\n",
    "log(f\"  Seeds: {SEEDS10}\")\n",
    "log(f\"  Device: {_detect_device(None)}\")\n",
    "log(f\"{'='*66}\")\n",
    "\n",
    "global_start_b = time.time()\n",
    "completed_b = len(seeds10_data.get(\"completed\", {}))\n",
    "trainer_ckpt_b = str(OUTPUT_DIR / SEEDS10_TRAINER_STATE)\n",
    "\n",
    "for exp_idx, (ds_name, algo, seed) in enumerate(seeds10_experiments, 1):\n",
    "    key = f\"{ds_name}_{algo}_s{seed}\"\n",
    "\n",
    "    if key in seeds10_data.get(\"completed\", {}):\n",
    "        continue\n",
    "\n",
    "    ds_info = IMAGING_DATASETS[ds_name]\n",
    "\n",
    "    elapsed = time.time() - global_start_b\n",
    "    remaining = total_seeds10 - completed_b\n",
    "    if completed_b > 0:\n",
    "        eta = str(timedelta(seconds=int(remaining * elapsed / completed_b)))\n",
    "    else:\n",
    "        eta = \"calculating...\"\n",
    "\n",
    "    log(f\"\\n--- [B:{exp_idx}/{total_seeds10}] {ds_name} | {algo} | s{seed} | ETA: {eta} ---\")\n",
    "\n",
    "    try:\n",
    "        result = run_single_experiment(\n",
    "            ds_name=ds_name, data_dir=ds_info[\"data_dir\"],\n",
    "            algorithm=algo, seed=seed, num_clients=5,\n",
    "            config=IMAGING_CONFIG, es_config=EARLY_STOPPING,\n",
    "            exp_idx=exp_idx, total_exps=total_seeds10,\n",
    "            checkpoint_data=seeds10_data, checkpoint_file=SEEDS10_CHECKPOINT,\n",
    "            exp_key=key, trainer_ckpt_path=trainer_ckpt_b,\n",
    "        )\n",
    "\n",
    "        seeds10_data[\"completed\"][key] = result\n",
    "        completed_b += 1\n",
    "        save_checkpoint(seeds10_data, SEEDS10_CHECKPOINT)\n",
    "\n",
    "        best_acc = result.get(\"best_metrics\", {}).get(\"accuracy\", 0)\n",
    "        es_info = f\" ES@R{result['actual_rounds']}\" if result.get(\"stopped_early\") else \"\"\n",
    "        log(f\"--- Done: Best={best_acc:.1%}{es_info} | {result['runtime_seconds']:.0f}s | [{completed_b}/{total_seeds10}] ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in {key}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        seeds10_data[\"completed\"][key] = {\n",
    "            \"dataset\": ds_name, \"algorithm\": algo, \"seed\": seed,\n",
    "            \"error\": str(e),\n",
    "        }\n",
    "        save_checkpoint(seeds10_data, SEEDS10_CHECKPOINT)\n",
    "\n",
    "elapsed_b = time.time() - global_start_b\n",
    "seeds10_data[\"metadata\"][\"end_time\"] = datetime.now().isoformat()\n",
    "seeds10_data[\"metadata\"][\"total_elapsed\"] = elapsed_b\n",
    "save_checkpoint(seeds10_data, SEEDS10_CHECKPOINT)\n",
    "\n",
    "log(f\"\\n{'='*66}\")\n",
    "log(f\"  PART B COMPLETED: {completed_b}/{total_seeds10}\")\n",
    "log(f\"  Total time: {timedelta(seconds=int(elapsed_b))}\")\n",
    "log(f\"{'='*66}\")\n",
    "\n",
    "if _log_file:\n",
    "    _log_file.close()\n",
    "    _log_file = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np\n",
    "\n",
    "# --- Part A: Scalability ---\n",
    "print('=' * 66)\n",
    "print('  PART A: Scalability Results')\n",
    "print('=' * 66)\n",
    "\n",
    "ckpt_a = f'{DRIVE_OUTPUT}/checkpoint_imaging_scalability.json'\n",
    "if os.path.exists(ckpt_a):\n",
    "    with open(ckpt_a) as f:\n",
    "        data_a = json.load(f)\n",
    "    completed_a = data_a.get('completed', {})\n",
    "    n_ok = sum(1 for v in completed_a.values() if 'error' not in v)\n",
    "    n_err = sum(1 for v in completed_a.values() if 'error' in v)\n",
    "    print(f'Completed: {n_ok}/{72} (errors: {n_err})')\n",
    "\n",
    "    header = f'{\"DS\":<14} {\"Algo\":<10} {\"K\":>4} {\"Best Acc\":>10} {\"Rounds\":>8}'\n",
    "    print(f'\\n{header}')\n",
    "    print('-' * len(header))\n",
    "    for ds in ['chest_xray', 'Brain_Tumor', 'Skin_Cancer']:\n",
    "        for algo in ['FedAvg', 'Ditto', 'HPFL']:\n",
    "            for k_val in [10, 20]:\n",
    "                accs = []\n",
    "                for seed in [42, 123, 456, 789]:\n",
    "                    key = f'{ds}_{algo}_K{k_val}_s{seed}'\n",
    "                    r = completed_a.get(key, {})\n",
    "                    if 'error' not in r and r:\n",
    "                        accs.append(r.get('best_metrics', {}).get('accuracy', 0))\n",
    "                if accs:\n",
    "                    print(f'{ds:<14} {algo:<10} {k_val:>4} {100*np.mean(accs):>9.1f}% {\"\":>8}')\n",
    "else:\n",
    "    print('No scalability checkpoint found.')\n",
    "\n",
    "# --- Part B: 10-Seed ---\n",
    "print(f'\\n{\"=\" * 66}')\n",
    "print('  PART B: 10-Seed Statistical Validation Results')\n",
    "print('=' * 66)\n",
    "\n",
    "ckpt_b = f'{DRIVE_OUTPUT}/checkpoint_imaging_seeds10.json'\n",
    "if os.path.exists(ckpt_b):\n",
    "    with open(ckpt_b) as f:\n",
    "        data_b = json.load(f)\n",
    "    completed_b = data_b.get('completed', {})\n",
    "    n_ok = sum(1 for v in completed_b.values() if 'error' not in v)\n",
    "    n_err = sum(1 for v in completed_b.values() if 'error' in v)\n",
    "    print(f'Completed: {n_ok}/{90} (errors: {n_err})')\n",
    "\n",
    "    header = f'{\"DS\":<14} {\"Algo\":<10} {\"Mean Acc\":>10} {\"Std\":>8} {\"Min\":>8} {\"Max\":>8} {\"ES\":>4}'\n",
    "    print(f'\\n{header}')\n",
    "    print('-' * len(header))\n",
    "    for ds in ['chest_xray', 'Brain_Tumor', 'Skin_Cancer']:\n",
    "        for algo in ['FedAvg', 'Ditto', 'HPFL']:\n",
    "            accs = []\n",
    "            es_count = 0\n",
    "            for seed in [42, 123, 456, 789, 999, 7, 13, 31, 67, 101]:\n",
    "                key = f'{ds}_{algo}_s{seed}'\n",
    "                r = completed_b.get(key, {})\n",
    "                if 'error' not in r and r:\n",
    "                    accs.append(r.get('best_metrics', {}).get('accuracy', 0))\n",
    "                    if r.get('stopped_early'):\n",
    "                        es_count += 1\n",
    "            if accs:\n",
    "                print(f'{ds:<14} {algo:<10} {100*np.mean(accs):>9.1f}% {100*np.std(accs):>7.1f}% {100*min(accs):>7.1f}% {100*max(accs):>7.1f}% {es_count:>3}x')\n",
    "            else:\n",
    "                print(f'{ds:<14} {algo:<10} {\"--\":>10} {\"--\":>8} {\"--\":>8} {\"--\":>8} {\"--\":>4}')\n",
    "\n",
    "    # Wilcoxon signed-rank test: HPFL vs FedAvg\n",
    "    print(f'\\n--- Wilcoxon Signed-Rank Test: HPFL vs FedAvg ---')\n",
    "    try:\n",
    "        from scipy.stats import wilcoxon\n",
    "        for ds in ['chest_xray', 'Brain_Tumor', 'Skin_Cancer']:\n",
    "            hpfl_accs = []\n",
    "            fedavg_accs = []\n",
    "            for seed in [42, 123, 456, 789, 999, 7, 13, 31, 67, 101]:\n",
    "                h = completed_b.get(f'{ds}_HPFL_s{seed}', {})\n",
    "                f = completed_b.get(f'{ds}_FedAvg_s{seed}', {})\n",
    "                if h and f and 'error' not in h and 'error' not in f:\n",
    "                    hpfl_accs.append(h.get('best_metrics', {}).get('accuracy', 0))\n",
    "                    fedavg_accs.append(f.get('best_metrics', {}).get('accuracy', 0))\n",
    "            if len(hpfl_accs) >= 5:\n",
    "                stat, p = wilcoxon(hpfl_accs, fedavg_accs, alternative='greater')\n",
    "                sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "                diff = 100 * (np.mean(hpfl_accs) - np.mean(fedavg_accs))\n",
    "                print(f'  {ds:<14} HPFL-FedAvg = {diff:+.1f}pp  p={p:.4f}  {sig}')\n",
    "            else:\n",
    "                print(f'  {ds:<14} insufficient data ({len(hpfl_accs)} pairs)')\n",
    "    except ImportError:\n",
    "        print('  scipy not available for Wilcoxon test')\n",
    "else:\n",
    "    print('No seeds10 checkpoint found yet.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "for fname in ['checkpoint_imaging_scalability.json',\n",
    "              'checkpoint_imaging_seeds10.json',\n",
    "              'experiment_imaging_scalability.log',\n",
    "              'experiment_imaging_seeds10.log']:\n",
    "    fpath = f'{DRIVE_OUTPUT}/{fname}'\n",
    "    if os.path.exists(fpath):\n",
    "        files.download(fpath)\n",
    "        print(f'Downloaded: {fname}')\n",
    "    else:\n",
    "        print(f'Not found: {fname}')"
   ]
  }
 ]
}