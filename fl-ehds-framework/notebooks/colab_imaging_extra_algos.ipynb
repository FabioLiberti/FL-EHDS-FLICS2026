{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL-EHDS: Imaging Extra Algorithms (FedProx + FedLESAM)\n",
    "\n",
    "Extends imaging experiments from 3 algorithms (FedAvg, Ditto, HPFL) to 5\n",
    "by adding **FedProx** and **FedLESAM**. Closes the algorithm gap between\n",
    "tabular (7 algos) and imaging (3 algos).\n",
    "\n",
    "**Setup:** Runtime > Change runtime type > **T4 GPU**\n",
    "\n",
    "**Experiments:** 2 algos × 3 datasets × 5 seeds = **30 experiments**\n",
    "\n",
    "**Checkpoint:** Saved to Google Drive after **every round**, per-client, per-epoch.\n",
    "If the session disconnects, re-run from Section 3 — it auto-resumes.\n",
    "\n",
    "**Estimated time:** ~2-3 hours on T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent checkpoint storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "DRIVE_OUTPUT = '/content/drive/MyDrive/FL-EHDS-FLICS2026/colab_results'\n",
    "os.makedirs(DRIVE_OUTPUT, exist_ok=True)\n",
    "print(f'Drive output: {DRIVE_OUTPUT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    mem = getattr(props, 'total_memory', None) or getattr(props, 'total_mem', 0)\n",
    "    print(f'Memory: {mem / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/FabioLiberti/FL-EHDS-FLICS2026.git /content/FL-EHDS-FLICS2026 2>/dev/null || (cd /content/FL-EHDS-FLICS2026 && git pull)\n",
    "%cd /content/FL-EHDS-FLICS2026/fl-ehds-framework\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q scikit-learn scipy tqdm Pillow structlog cryptography grpcio aiohttp pydantic pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kagglehub\n",
    "\n",
    "import os\n",
    "os.environ['KAGGLE_API_TOKEN'] = 'KGAT_edd561c1bc682c9ad06930bacd164431'\n",
    "\n",
    "import kagglehub\n",
    "print(f'kagglehub version: {kagglehub.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import shutil, glob\n",
    "\n",
    "cache_path = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
    "os.makedirs('data/chest_xray', exist_ok=True)\n",
    "for item in ['train', 'test', 'val']:\n",
    "    src = os.path.join(cache_path, 'chest_xray', item)\n",
    "    if not os.path.exists(src):\n",
    "        src = os.path.join(cache_path, item)\n",
    "    dst = f'data/chest_xray/{item}'\n",
    "    if os.path.exists(src) and not os.path.exists(dst):\n",
    "        shutil.copytree(src, dst)\n",
    "shutil.rmtree('data/chest_xray/__MACOSX', ignore_errors=True)\n",
    "print('Chest X-Ray ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cache_path = kagglehub.dataset_download(\"fanconic/skin-cancer-malignant-vs-benign\")\n",
    "dst = 'data/Skin Cancer'\n",
    "if not os.path.exists(dst):\n",
    "    shutil.copytree(cache_path, dst)\n",
    "print('Skin Cancer ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cache_path = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")\n",
    "os.makedirs('data/Brain_Tumor', exist_ok=True)\n",
    "for root, dirs, files in os.walk(cache_path):\n",
    "    for d in dirs:\n",
    "        d_lower = d.lower()\n",
    "        if d_lower in ['glioma', 'meningioma', 'pituitary', 'notumor', 'no_tumor', 'healthy']:\n",
    "            target = 'healthy' if d_lower in ['notumor', 'no_tumor'] else d_lower\n",
    "            src = os.path.join(root, d)\n",
    "            dst_dir = f'data/Brain_Tumor/{target}'\n",
    "            if not os.path.exists(dst_dir):\n",
    "                shutil.copytree(src, dst_dir)\n",
    "            else:\n",
    "                for f in os.listdir(src):\n",
    "                    sf, df = os.path.join(src, f), os.path.join(dst_dir, f)\n",
    "                    if os.path.isfile(sf) and not os.path.exists(df):\n",
    "                        shutil.copy2(sf, df)\n",
    "print('Brain Tumor ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Dataset Summary ===')\n",
    "for ds_name, ds_path in [('Chest X-Ray', 'data/chest_xray'),\n",
    "                          ('Skin Cancer', 'data/Skin Cancer'),\n",
    "                          ('Brain Tumor', 'data/Brain_Tumor')]:\n",
    "    count = sum(1 for _ in glob.iglob(f'{ds_path}/**/*.*', recursive=True)\n",
    "                if _.lower().endswith(('.jpg', '.jpeg', '.png')))\n",
    "    subdirs = [d for d in os.listdir(ds_path) if os.path.isdir(os.path.join(ds_path, d))]\n",
    "    print(f'  {ds_name:15s}: {count:5d} images, classes: {subdirs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Extra Algorithm Experiments\n",
    "\n",
    "**30 experiments** = 2 algos (FedProx, FedLESAM) × 3 datasets × 5 seeds\n",
    "\n",
    "Checkpoint saved to Drive after **every training round**, with per-client and per-epoch metrics.\n",
    "If Colab disconnects, re-run this cell — it auto-resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import tempfile\n",
    "import traceback\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, '/content/FL-EHDS-FLICS2026/fl-ehds-framework')\n",
    "\n",
    "from terminal.fl_trainer import ImageFederatedTrainer, _detect_device\n",
    "\n",
    "# ======================================================================\n",
    "# Configuration\n",
    "# ======================================================================\n",
    "\n",
    "ALGORITHMS = [\"FedProx\", \"FedLESAM\"]\n",
    "SEEDS = [42, 123, 456, 789, 999]\n",
    "\n",
    "IMAGING_DATASETS = {\n",
    "    \"chest_xray\": {\"data_dir\": \"data/chest_xray\", \"num_classes\": 2, \"short\": \"CX\"},\n",
    "    \"Brain_Tumor\": {\"data_dir\": \"data/Brain_Tumor\", \"num_classes\": 4, \"short\": \"BT\"},\n",
    "    \"Skin_Cancer\": {\"data_dir\": \"data/Skin Cancer\", \"num_classes\": 2, \"short\": \"SC\"},\n",
    "}\n",
    "\n",
    "IMAGING_CONFIG = dict(\n",
    "    num_clients=5, num_rounds=20, local_epochs=2, batch_size=32,\n",
    "    learning_rate=0.001, model_type=\"resnet18\", is_iid=False, alpha=0.5,\n",
    "    freeze_backbone=False, freeze_level=2, use_fedbn=True,\n",
    "    use_class_weights=True, use_amp=True, mu=0.1,\n",
    ")\n",
    "\n",
    "DATASET_OVERRIDES = {\"Brain_Tumor\": {\"learning_rate\": 0.0005}}\n",
    "\n",
    "EARLY_STOPPING = dict(enabled=True, patience=4, min_delta=0.003, min_rounds=8, metric=\"accuracy\")\n",
    "\n",
    "OUTPUT_DIR = Path(DRIVE_OUTPUT)\n",
    "CHECKPOINT_FILE = \"checkpoint_imaging_extra_algos.json\"\n",
    "LOG_FILE = \"experiment_imaging_extra_algos.log\"\n",
    "TRAINER_STATE_FILE = \".trainer_state_extra_algos.pt\"\n",
    "\n",
    "print(f\"Device: {_detect_device(None)}\")\n",
    "print(f\"Algorithms: {ALGORITHMS}\")\n",
    "print(f\"Experiments: {len(ALGORITHMS)} algos x {len(IMAGING_DATASETS)} ds x {len(SEEDS)} seeds = {len(ALGORITHMS)*len(IMAGING_DATASETS)*len(SEEDS)}\")\n",
    "print(f\"Checkpoint: {OUTPUT_DIR / CHECKPOINT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Utilities\n",
    "# ======================================================================\n",
    "\n",
    "_log_file = None\n",
    "\n",
    "def log(msg, also_print=True):\n",
    "    ts = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    line = f\"[{ts}] {msg}\"\n",
    "    if also_print:\n",
    "        print(line, flush=True)\n",
    "    if _log_file:\n",
    "        try:\n",
    "            _log_file.write(line + \"\\n\")\n",
    "            _log_file.flush()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def save_checkpoint(data):\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    path = OUTPUT_DIR / CHECKPOINT_FILE\n",
    "    bak = OUTPUT_DIR / (CHECKPOINT_FILE + \".bak\")\n",
    "    data[\"metadata\"][\"last_save\"] = datetime.now().isoformat()\n",
    "    fd, tmp = tempfile.mkstemp(dir=str(OUTPUT_DIR), prefix=\".ckpt_xalgo_\", suffix=\".tmp\")\n",
    "    try:\n",
    "        with os.fdopen(fd, \"w\") as f:\n",
    "            json.dump(data, f, indent=2, default=str)\n",
    "            f.flush()\n",
    "            os.fsync(f.fileno())\n",
    "        if path.exists():\n",
    "            shutil.copy2(str(path), str(bak))\n",
    "        os.replace(tmp, str(path))\n",
    "    except Exception:\n",
    "        try:\n",
    "            os.unlink(tmp)\n",
    "        except OSError:\n",
    "            pass\n",
    "        raise\n",
    "\n",
    "def load_checkpoint():\n",
    "    for p in [OUTPUT_DIR / CHECKPOINT_FILE, OUTPUT_DIR / (CHECKPOINT_FILE + \".bak\")]:\n",
    "        if p.exists():\n",
    "            try:\n",
    "                with open(p) as f:\n",
    "                    return json.load(f)\n",
    "            except (json.JSONDecodeError, IOError):\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def _cleanup_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "class EarlyStoppingMonitor:\n",
    "    def __init__(self, patience=4, min_delta=0.003, min_rounds=8, metric=\"accuracy\"):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.min_rounds = min_rounds\n",
    "        self.metric = metric\n",
    "        self.best_value = -float('inf')\n",
    "        self.best_round = 0\n",
    "        self.counter = 0\n",
    "\n",
    "    def check(self, round_num, metrics):\n",
    "        value = metrics.get(self.metric, 0)\n",
    "        if value > self.best_value + self.min_delta:\n",
    "            self.best_value = value\n",
    "            self.best_round = round_num\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        if round_num < self.min_rounds:\n",
    "            return False\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "def _evaluate_per_client(trainer):\n",
    "    model = trainer.global_model\n",
    "    model.eval()\n",
    "    per_client = {}\n",
    "    with torch.no_grad():\n",
    "        for cid in range(trainer.num_clients):\n",
    "            X, y = trainer.client_test_data[cid]\n",
    "            X_t = torch.FloatTensor(X).to(trainer.device) if isinstance(X, np.ndarray) else X.to(trainer.device)\n",
    "            y_t = torch.LongTensor(y).to(trainer.device) if isinstance(y, np.ndarray) else y.to(trainer.device)\n",
    "            correct = total = 0\n",
    "            for i in range(0, len(y_t), 64):\n",
    "                out = model(X_t[i:i+64])\n",
    "                preds = out.argmax(dim=1)\n",
    "                correct += (preds == y_t[i:i+64]).sum().item()\n",
    "                total += len(y_t[i:i+64])\n",
    "            per_client[str(cid)] = correct / total if total > 0 else 0.0\n",
    "    return per_client\n",
    "\n",
    "def _compute_fairness(per_client_acc):\n",
    "    accs = list(per_client_acc.values())\n",
    "    if not accs:\n",
    "        return {}\n",
    "    jain = (sum(accs)**2) / (len(accs) * sum(a**2 for a in accs)) if accs else 0\n",
    "    sorted_a = sorted(accs)\n",
    "    n = len(sorted_a)\n",
    "    cumsum = np.cumsum(sorted_a)\n",
    "    gini = (2 * sum((i+1)*v for i, v in enumerate(sorted_a))) / (n * cumsum[-1]) - (n+1)/n if cumsum[-1] > 0 else 0\n",
    "    return {\n",
    "        \"mean\": round(float(np.mean(accs)), 4),\n",
    "        \"std\": round(float(np.std(accs)), 4),\n",
    "        \"min\": round(float(min(accs)), 4),\n",
    "        \"max\": round(float(max(accs)), 4),\n",
    "        \"jain_index\": round(float(jain), 4),\n",
    "        \"gini\": round(float(max(0, gini)), 4),\n",
    "    }\n",
    "\n",
    "print('Utilities loaded OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Training function\n",
    "# ======================================================================\n",
    "\n",
    "def run_single_experiment(\n",
    "    ds_name, data_dir, algorithm, seed,\n",
    "    config, es_config,\n",
    "    exp_idx, total_exps,\n",
    "    checkpoint_data=None, exp_key=None,\n",
    "    trainer_ckpt_path=None,\n",
    "):\n",
    "    start = time.time()\n",
    "    num_rounds = config[\"num_rounds\"]\n",
    "\n",
    "    cfg = {**config}\n",
    "    if ds_name in DATASET_OVERRIDES:\n",
    "        cfg.update(DATASET_OVERRIDES[ds_name])\n",
    "\n",
    "    trainer = ImageFederatedTrainer(\n",
    "        data_dir=data_dir,\n",
    "        num_clients=cfg[\"num_clients\"],\n",
    "        algorithm=algorithm,\n",
    "        local_epochs=cfg[\"local_epochs\"],\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        learning_rate=cfg[\"learning_rate\"],\n",
    "        is_iid=cfg[\"is_iid\"],\n",
    "        alpha=cfg[\"alpha\"],\n",
    "        mu=cfg.get(\"mu\", 0.1),\n",
    "        seed=seed,\n",
    "        model_type=cfg[\"model_type\"],\n",
    "        freeze_backbone=cfg.get(\"freeze_backbone\", False),\n",
    "        freeze_level=cfg.get(\"freeze_level\"),\n",
    "        use_fedbn=cfg.get(\"use_fedbn\", False),\n",
    "        use_class_weights=cfg.get(\"use_class_weights\", True),\n",
    "        use_amp=cfg.get(\"use_amp\", True),\n",
    "    )\n",
    "    trainer.num_rounds = num_rounds\n",
    "\n",
    "    es = EarlyStoppingMonitor(\n",
    "        **{k: v for k, v in es_config.items() if k != \"enabled\"}\n",
    "    ) if es_config.get(\"enabled\") else None\n",
    "\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "    best_round = 0\n",
    "    start_round = 0\n",
    "\n",
    "    # Resume mid-experiment\n",
    "    in_prog = checkpoint_data.get(\"in_progress\") if checkpoint_data else None\n",
    "    if (in_prog and in_prog.get(\"key\") == exp_key\n",
    "            and trainer_ckpt_path and Path(trainer_ckpt_path).exists()):\n",
    "        try:\n",
    "            start_round = trainer.load_checkpoint(trainer_ckpt_path)\n",
    "            history = in_prog.get(\"history\", [])\n",
    "            best_acc = in_prog.get(\"best_acc\", 0.0)\n",
    "            best_round = in_prog.get(\"best_round\", 0)\n",
    "            if es and history:\n",
    "                for h in history:\n",
    "                    es.check(h[\"round\"], {\"accuracy\": h[\"accuracy\"]})\n",
    "            log(f\"  RESUMED from round {start_round} (best={best_acc:.1%})\")\n",
    "        except Exception as e:\n",
    "            log(f\"  WARNING: resume failed ({e}), restarting from R1\")\n",
    "            start_round = 0\n",
    "            history = []\n",
    "            best_acc = 0.0\n",
    "            best_round = 0\n",
    "\n",
    "    for r in range(start_round, num_rounds):\n",
    "        rr = trainer.train_round(r)\n",
    "\n",
    "        # Per-client + per-epoch metrics\n",
    "        client_metrics = [\n",
    "            {\n",
    "                \"client_id\": cr.client_id,\n",
    "                \"train_loss\": round(cr.train_loss, 6),\n",
    "                \"train_acc\": round(cr.train_acc, 6),\n",
    "                \"num_samples\": cr.num_samples,\n",
    "                \"epochs_completed\": cr.epochs_completed,\n",
    "                \"epoch_metrics\": cr.epoch_metrics or [],\n",
    "            }\n",
    "            for cr in rr.client_results\n",
    "        ]\n",
    "\n",
    "        metrics = {\n",
    "            \"round\": r + 1,\n",
    "            \"accuracy\": rr.global_acc,\n",
    "            \"loss\": rr.global_loss,\n",
    "            \"f1\": rr.global_f1,\n",
    "            \"precision\": rr.global_precision,\n",
    "            \"recall\": rr.global_recall,\n",
    "            \"auc\": rr.global_auc,\n",
    "            \"time_seconds\": round(rr.time_seconds, 2),\n",
    "            \"client_results\": client_metrics,\n",
    "        }\n",
    "        history.append(metrics)\n",
    "\n",
    "        if rr.global_acc > best_acc:\n",
    "            best_acc = rr.global_acc\n",
    "            best_round = r + 1\n",
    "\n",
    "        log(f\"[{exp_idx}/{total_exps}] {ds_name} | {algorithm} | s{seed} | \"\n",
    "            f\"R{r+1}/{num_rounds} | Acc:{rr.global_acc:.1%} | Best:{best_acc:.1%}(r{best_round})\")\n",
    "\n",
    "        # Save checkpoint after EVERY round: trainer .pt + JSON\n",
    "        if checkpoint_data is not None and exp_key:\n",
    "            if trainer_ckpt_path:\n",
    "                try:\n",
    "                    trainer.save_checkpoint(trainer_ckpt_path)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            checkpoint_data[\"in_progress\"] = {\n",
    "                \"key\": exp_key,\n",
    "                \"dataset\": ds_name,\n",
    "                \"algorithm\": algorithm,\n",
    "                \"seed\": seed,\n",
    "                \"round\": r + 1,\n",
    "                \"total_rounds\": num_rounds,\n",
    "                \"best_acc\": best_acc,\n",
    "                \"best_round\": best_round,\n",
    "                \"history\": history,\n",
    "                \"elapsed_seconds\": round(time.time() - start, 1),\n",
    "            }\n",
    "            save_checkpoint(checkpoint_data)\n",
    "\n",
    "        if es and es.check(r + 1, {\"accuracy\": rr.global_acc}):\n",
    "            log(f\"  -> Early stop at R{r+1} (best={best_acc:.1%} at r{best_round})\")\n",
    "            break\n",
    "\n",
    "    per_client_acc = _evaluate_per_client(trainer)\n",
    "    fairness = _compute_fairness(per_client_acc)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    result = {\n",
    "        \"dataset\": ds_name,\n",
    "        \"algorithm\": algorithm,\n",
    "        \"seed\": seed,\n",
    "        \"history\": history,\n",
    "        \"final_metrics\": history[-1] if history else {},\n",
    "        \"per_client_acc\": per_client_acc,\n",
    "        \"fairness\": fairness,\n",
    "        \"runtime_seconds\": round(elapsed, 1),\n",
    "        \"config\": cfg,\n",
    "        \"stopped_early\": es is not None and es.counter >= es.patience,\n",
    "        \"actual_rounds\": len(history),\n",
    "        \"best_metrics\": {\"accuracy\": best_acc, \"round\": best_round},\n",
    "        \"best_round\": best_round,\n",
    "    }\n",
    "\n",
    "    if checkpoint_data is not None:\n",
    "        checkpoint_data.pop(\"in_progress\", None)\n",
    "    if trainer_ckpt_path:\n",
    "        try:\n",
    "            Path(trainer_ckpt_path).unlink(missing_ok=True)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "    del trainer\n",
    "    _cleanup_gpu()\n",
    "    return result\n",
    "\n",
    "print('Training function loaded OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# MAIN EXPERIMENT LOOP\n",
    "# ======================================================================\n",
    "\n",
    "_log_file = open(OUTPUT_DIR / LOG_FILE, \"a\")\n",
    "\n",
    "experiments = []\n",
    "for ds_name in IMAGING_DATASETS:\n",
    "    for algo in ALGORITHMS:\n",
    "        for seed in SEEDS:\n",
    "            experiments.append((ds_name, algo, seed))\n",
    "\n",
    "total_exps = len(experiments)\n",
    "\n",
    "checkpoint_data = load_checkpoint()\n",
    "if checkpoint_data:\n",
    "    done = len(checkpoint_data.get(\"completed\", {}))\n",
    "    log(f\"AUTO-RESUMED: {done}/{total_exps} completed\")\n",
    "else:\n",
    "    checkpoint_data = {\n",
    "        \"completed\": {},\n",
    "        \"metadata\": {\n",
    "            \"total_experiments\": total_exps,\n",
    "            \"purpose\": \"Imaging extra algorithms: FedProx + FedLESAM on ResNet18 (closing algorithm gap with tabular)\",\n",
    "            \"algorithms\": ALGORITHMS,\n",
    "            \"datasets\": list(IMAGING_DATASETS.keys()),\n",
    "            \"seeds\": SEEDS,\n",
    "            \"start_time\": datetime.now().isoformat(),\n",
    "            \"last_save\": None,\n",
    "            \"version\": \"imaging_extra_algos_v1\",\n",
    "        }\n",
    "    }\n",
    "\n",
    "log(f\"\\n{'='*66}\")\n",
    "log(f\"  FL-EHDS Imaging — Extra Algorithms (FedProx + FedLESAM)\")\n",
    "log(f\"  {total_exps} experiments = {len(ALGORITHMS)} algos x {len(IMAGING_DATASETS)} DS x {len(SEEDS)} seeds\")\n",
    "log(f\"  Algorithms: {ALGORITHMS}\")\n",
    "log(f\"  Device: {_detect_device(None)}\")\n",
    "log(f\"{'='*66}\")\n",
    "\n",
    "global_start = time.time()\n",
    "completed_count = len(checkpoint_data.get(\"completed\", {}))\n",
    "trainer_ckpt_path = str(OUTPUT_DIR / TRAINER_STATE_FILE)\n",
    "\n",
    "for exp_idx, (ds_name, algo, seed) in enumerate(experiments, 1):\n",
    "    key = f\"{ds_name}_{algo}_s{seed}\"\n",
    "\n",
    "    if key in checkpoint_data.get(\"completed\", {}):\n",
    "        continue\n",
    "\n",
    "    ds_info = IMAGING_DATASETS[ds_name]\n",
    "\n",
    "    elapsed = time.time() - global_start\n",
    "    if completed_count > 0:\n",
    "        eta = str(timedelta(seconds=int((total_exps - completed_count) * elapsed / completed_count)))\n",
    "    else:\n",
    "        eta = \"calculating...\"\n",
    "\n",
    "    log(f\"\\n--- [{exp_idx}/{total_exps}] {ds_name} | {algo} | seed={seed} | ETA: {eta} ---\")\n",
    "\n",
    "    try:\n",
    "        result = run_single_experiment(\n",
    "            ds_name=ds_name, data_dir=ds_info[\"data_dir\"],\n",
    "            algorithm=algo, seed=seed,\n",
    "            config=IMAGING_CONFIG, es_config=EARLY_STOPPING,\n",
    "            exp_idx=exp_idx, total_exps=total_exps,\n",
    "            checkpoint_data=checkpoint_data, exp_key=key,\n",
    "            trainer_ckpt_path=trainer_ckpt_path,\n",
    "        )\n",
    "\n",
    "        checkpoint_data[\"completed\"][key] = result\n",
    "        completed_count += 1\n",
    "        save_checkpoint(checkpoint_data)\n",
    "\n",
    "        best_acc = result.get(\"best_metrics\", {}).get(\"accuracy\", 0)\n",
    "        es_info = f\" ES@R{result['actual_rounds']}\" if result.get(\"stopped_early\") else \"\"\n",
    "        log(f\"--- Done: Best={best_acc:.1%}{es_info} | {result['runtime_seconds']:.0f}s | [{completed_count}/{total_exps}] ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in {key}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        checkpoint_data[\"completed\"][key] = {\n",
    "            \"dataset\": ds_name, \"algorithm\": algo, \"seed\": seed,\n",
    "            \"error\": str(e),\n",
    "        }\n",
    "        save_checkpoint(checkpoint_data)\n",
    "\n",
    "elapsed_total = time.time() - global_start\n",
    "checkpoint_data[\"metadata\"][\"end_time\"] = datetime.now().isoformat()\n",
    "checkpoint_data[\"metadata\"][\"total_elapsed\"] = elapsed_total\n",
    "save_checkpoint(checkpoint_data)\n",
    "\n",
    "log(f\"\\n{'='*66}\")\n",
    "log(f\"  COMPLETED: {completed_count}/{total_exps}\")\n",
    "log(f\"  Total time: {timedelta(seconds=int(elapsed_total))}\")\n",
    "log(f\"{'='*66}\")\n",
    "\n",
    "if _log_file:\n",
    "    _log_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check Progress & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np\n",
    "\n",
    "ckpt_path = f'{DRIVE_OUTPUT}/checkpoint_imaging_extra_algos.json'\n",
    "if os.path.exists(ckpt_path):\n",
    "    with open(ckpt_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    completed = data.get('completed', {})\n",
    "    n_ok = sum(1 for v in completed.values() if 'error' not in v)\n",
    "    n_err = sum(1 for v in completed.values() if 'error' in v)\n",
    "    total = data.get('metadata', {}).get('total_experiments', '?')\n",
    "\n",
    "    print(f'Completed: {n_ok}/{total} (errors: {n_err})')\n",
    "\n",
    "    in_prog = data.get('in_progress', {})\n",
    "    if in_prog:\n",
    "        print(f'In progress: {in_prog.get(\"key\", \"?\")} '\n",
    "              f'round {in_prog.get(\"round\", \"?\")}/{in_prog.get(\"total_rounds\", \"?\")}')\n",
    "\n",
    "    header = f'{\"DS\":<14} {\"Algo\":<12} {\"Best Acc\":>10} {\"Rounds\":>8} {\"ES\":>4}'\n",
    "    print(f'\\n{header}')\n",
    "    print('-' * len(header))\n",
    "\n",
    "    for ds in ['chest_xray', 'Brain_Tumor', 'Skin_Cancer']:\n",
    "        for algo in ['FedProx', 'FedLESAM']:\n",
    "            accs = []\n",
    "            rounds_list = []\n",
    "            es_count = 0\n",
    "            for seed in [42, 123, 456, 789, 999]:\n",
    "                k = f'{ds}_{algo}_s{seed}'\n",
    "                r = completed.get(k, {})\n",
    "                if 'error' not in r and r:\n",
    "                    accs.append(r.get('best_metrics', {}).get('accuracy', 0))\n",
    "                    rounds_list.append(r.get('actual_rounds', 0))\n",
    "                    if r.get('stopped_early'):\n",
    "                        es_count += 1\n",
    "            if accs:\n",
    "                print(f'{ds:<14} {algo:<12} {100*np.mean(accs):>9.1f}% {np.mean(rounds_list):>7.0f}R {es_count:>3}x')\n",
    "            else:\n",
    "                print(f'{ds:<14} {algo:<12} {\"--\":>10} {\"--\":>8} {\"--\":>4}')\n",
    "else:\n",
    "    print('No checkpoint found yet.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "ckpt_path = f'{DRIVE_OUTPUT}/checkpoint_imaging_extra_algos.json'\n",
    "if os.path.exists(ckpt_path):\n",
    "    files.download(ckpt_path)\n",
    "    print('Downloaded: checkpoint_imaging_extra_algos.json')\n",
    "    log_path = f'{DRIVE_OUTPUT}/experiment_imaging_extra_algos.log'\n",
    "    if os.path.exists(log_path):\n",
    "        files.download(log_path)\n",
    "else:\n",
    "    print('No checkpoint to download yet.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
